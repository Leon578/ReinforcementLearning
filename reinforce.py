# -*- coding: utf-8 -*-
"""REINFORCE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GAvtHXoyHb6KsxlRj1_Y9AzeIvJsHEj9
"""

import tensorflow as tf 
import tensorflow_probability as tfp 
from keras import layers
from keras.models import Model 
from keras.optimizers import Adam
import gym

env = gym.make('CartPole-v0')

class Actor(tf.keras.Model):
  def __init__(self, num_actions, num_hidden_units):
    super(Actor, self).__init__() 

    self.shared_1 = layers.Dense(num_hidden_units, activation = "relu")
    self.actor = layers.Dense(num_actions)

  def call(self, input_obs):
    x = self.shared_1(input_obs)
    return self.actor(x)

policy = Actor(num_actions = 2, num_hidden_units = 128)

def complete_loss( action_probs, rewards):
    action_log_probs = tf.math.log(action_probs)
    action_loss = -tf.math.reduce_sum(action_log_probs*rewards)
    return action_loss


#We need a neural network that will be able to ]
optimizer = Adam(learning_rate = 0.001)
def step_episode(env, policy):
  state = env.reset()
  rewards = 0
  done = False
  action_probabilities_list = []
  while not done:
    state = tf.expand_dims(state, 0)
    action_logits = policy.call(state.numpy()) # choose a random action
    discrete_dist = tfp.distributions.Categorical(logits=action_logits)
    #Get the action from the distribution 
    action_val = discrete_dist.sample() 
    
    action_probs = discrete_dist.prob(action_val)
    action_probabilities_list.append(action_probs)
  
    state, reward, done, _ = env.step(action_val.numpy()[0])
    rewards = reward + rewards
  env.close()
  return action_probabilities_list, rewards

for episode in range(2000): 
      with tf.GradientTape() as tape:
        action_probabilities_list, rewards = step_episode(env, policy)
        loss = complete_loss(action_probabilities_list, rewards)
        if episode%20 == 0 :
          #Maybe print the average rewards? 
          print(rewards) 
        gradient = tape.gradient(loss, policy.trainable_variables)
        optimizer.apply_gradients(zip(gradient, policy.trainable_variables))
      rewards = 0
      action_probabilities_list = []
      state = env.reset()