{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6Z4FIiQVtrKpc2N4NTuSf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leon578/ReinforcementLearning/blob/main/LSTM_Optimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wo7jY19_busP",
        "outputId": "e615f66d-70d1-454a-bca4-107a77493550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "--- Starting Deposit Management Simulation (RNN-Based State) ---\n",
            "Using device: cpu\n",
            "\n",
            "--- Parameters ---\n",
            "  Deposit_Type  Lifetime  Cost  Type_Index\n",
            "0       Type_A        10   100           0\n",
            "1       Type_B        20   250           1\n",
            "2       Type_C        30   500           2\n",
            "3       Type_D        40   750           3\n",
            "4       Type_E        60  1000           4\n",
            "\n",
            "Initial Records: 50, Initial Total Amount: 427,653.41\n",
            "--------------------------------------------------\n",
            "\n",
            "Initializing PPO Agent (RNN): Features=4, Embedding Dim=128, Action Dim=5\n",
            "PPO Agent (RNN) created successfully.\n",
            "\n",
            "Starting training for 3000 episodes...\n",
            "Update trigger: 1024 steps. Batch: 128, Epochs: 8\n",
            "Fitting scaler on data from first 20 episodes...\n",
            "Fitting scaler on 312410 samples...\n",
            "Scaler fit successfully.\n",
            "Scaler Mean: [1.83206224e+01 6.70758298e+02 3.14843582e+03 2.65399955e+00]\n",
            "Scaler Scale (StdDev): [1.44785304e+01 3.03937359e+02 4.21529230e+03 1.27333685e+00]\n",
            "Ep 25/3000 | Avg R: 1.9 | Avg WCost: 670.39 | Avg Div: 1.467 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.025 | V Loss: 0.130 | Entropy: -3.402\n",
            "Ep 50/3000 | Avg R: 6.0 | Avg WCost: 636.91 | Avg Div: 1.515 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.026 | V Loss: 0.135 | Entropy: -3.528\n",
            "Ep 75/3000 | Avg R: 10.4 | Avg WCost: 594.14 | Avg Div: 1.556 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.025 | V Loss: 0.104 | Entropy: -3.893\n",
            "Ep 100/3000 | Avg R: 14.1 | Avg WCost: 546.61 | Avg Div: 1.578 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.030 | V Loss: 0.106 | Entropy: -4.471\n",
            "Ep 125/3000 | Avg R: 16.5 | Avg WCost: 511.78 | Avg Div: 1.585 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.028 | V Loss: 0.070 | Entropy: -5.168\n",
            "Ep 150/3000 | Avg R: 18.2 | Avg WCost: 475.68 | Avg Div: 1.575 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.017 | V Loss: 0.049 | Entropy: -5.829\n",
            "Ep 175/3000 | Avg R: 18.6 | Avg WCost: 447.02 | Avg Div: 1.548 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.012 | V Loss: 0.042 | Entropy: -6.367\n",
            "Ep 200/3000 | Avg R: 18.8 | Avg WCost: 427.71 | Avg Div: 1.530 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.004 | V Loss: 0.051 | Entropy: -6.713\n",
            "Ep 225/3000 | Avg R: 18.6 | Avg WCost: 412.74 | Avg Div: 1.512 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.006 | V Loss: 0.051 | Entropy: -7.092\n",
            "Ep 250/3000 | Avg R: 18.5 | Avg WCost: 401.84 | Avg Div: 1.493 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.006 | V Loss: 0.063 | Entropy: -7.380\n",
            "Ep 275/3000 | Avg R: 18.4 | Avg WCost: 399.51 | Avg Div: 1.491 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.004 | V Loss: 0.064 | Entropy: -7.459\n",
            "Ep 300/3000 | Avg R: 18.3 | Avg WCost: 394.29 | Avg Div: 1.483 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.004 | V Loss: 0.068 | Entropy: -7.538\n",
            "Ep 325/3000 | Avg R: 18.3 | Avg WCost: 394.55 | Avg Div: 1.486 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.004 | V Loss: 0.063 | Entropy: -7.400\n",
            "Ep 350/3000 | Avg R: 18.5 | Avg WCost: 397.33 | Avg Div: 1.493 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.004 | V Loss: 0.056 | Entropy: -7.413\n",
            "Ep 375/3000 | Avg R: 18.2 | Avg WCost: 390.67 | Avg Div: 1.483 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.005 | V Loss: 0.064 | Entropy: -7.439\n",
            "Ep 400/3000 | Avg R: 18.4 | Avg WCost: 395.08 | Avg Div: 1.492 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.006 | V Loss: 0.064 | Entropy: -7.426\n",
            "Ep 425/3000 | Avg R: 18.4 | Avg WCost: 393.19 | Avg Div: 1.486 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.003 | V Loss: 0.069 | Entropy: -7.481\n",
            "Ep 450/3000 | Avg R: 18.4 | Avg WCost: 396.10 | Avg Div: 1.489 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.003 | V Loss: 0.059 | Entropy: -7.433\n",
            "Ep 475/3000 | Avg R: 18.5 | Avg WCost: 398.73 | Avg Div: 1.495 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.003 | V Loss: 0.066 | Entropy: -7.433\n",
            "Ep 500/3000 | Avg R: 18.7 | Avg WCost: 404.71 | Avg Div: 1.506 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.004 | V Loss: 0.055 | Entropy: -7.282\n",
            "Ep 525/3000 | Avg R: 18.6 | Avg WCost: 399.39 | Avg Div: 1.498 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.006 | V Loss: 0.057 | Entropy: -7.459\n",
            "Ep 550/3000 | Avg R: 18.5 | Avg WCost: 397.84 | Avg Div: 1.497 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.005 | V Loss: 0.056 | Entropy: -7.479\n",
            "Ep 575/3000 | Avg R: 18.6 | Avg WCost: 400.22 | Avg Div: 1.503 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.005 | V Loss: 0.053 | Entropy: -7.418\n",
            "Ep 600/3000 | Avg R: 18.6 | Avg WCost: 397.08 | Avg Div: 1.499 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.004 | V Loss: 0.054 | Entropy: -7.634\n",
            "Ep 625/3000 | Avg R: 18.7 | Avg WCost: 399.69 | Avg Div: 1.503 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.004 | V Loss: 0.050 | Entropy: -7.696\n",
            "Ep 650/3000 | Avg R: 18.5 | Avg WCost: 391.54 | Avg Div: 1.489 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.001 | V Loss: 0.059 | Entropy: -7.760\n",
            "Ep 675/3000 | Avg R: 18.4 | Avg WCost: 390.35 | Avg Div: 1.488 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.003 | V Loss: 0.064 | Entropy: -7.723\n",
            "Ep 700/3000 | Avg R: 18.6 | Avg WCost: 397.75 | Avg Div: 1.498 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.001 | V Loss: 0.058 | Entropy: -7.582\n",
            "Ep 725/3000 | Avg R: 18.4 | Avg WCost: 389.39 | Avg Div: 1.485 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.004 | V Loss: 0.057 | Entropy: -7.770\n",
            "Ep 750/3000 | Avg R: 18.3 | Avg WCost: 385.60 | Avg Div: 1.477 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.001 | V Loss: 0.068 | Entropy: -7.866\n",
            "Ep 775/3000 | Avg R: 18.4 | Avg WCost: 387.03 | Avg Div: 1.476 | Avg Len: 120.0 | Avg Fin Amt: 427,653 | P Loss: -0.006 | V Loss: 0.064 | Entropy: -7.887\n",
            "\n",
            "--- Training Interrupted ---\n",
            "\n",
            "--- Skipping Training Plot (No Results) ---\n",
            "\n",
            "--- Evaluating Final Agent ---\n",
            "\n",
            "--- Starting Evaluation ---\n",
            "End Eval Ep 20 (Steps: 120): Rwd=18.4 | WCost=352.64 | Div=1.459 | Amt=427,653\n",
            "\n",
            "--- Evaluation Summary (20 episodes) ---\n",
            "Avg Reward: 18.43 +/- 0.00\n",
            "Avg Final WCost: 352.64\n",
            "Avg Final Div: 1.4586\n",
            "Avg Final Amt: 427,653\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "--- Plotting Evaluation Distribution (First Episode) ---\n",
            "\n",
            "Displaying generated plots...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment closed.\n",
            "\n",
            "--- Script Finished ---\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# --- Imports ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Dirichlet\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from typing import Tuple, Dict, List, Optional, Any, Union\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler # For normalization\n",
        "\n",
        "# --- Data Generation (No changes needed) ---\n",
        "\n",
        "def create_parameters_df() -> pd.DataFrame:\n",
        "    data = {\n",
        "        'Deposit_Type': ['Type_A', 'Type_B', 'Type_C', 'Type_D', 'Type_E'],\n",
        "        'Lifetime': [10, 20, 30, 40, 60], # Lifetimes in months (steps)\n",
        "        'Cost': [100, 250, 500, 750, 1000] # Cost per unit amount (e.g., per $1000)\n",
        "    }\n",
        "    parameters_df = pd.DataFrame(data)\n",
        "    parameters_df['Type_Index'] = parameters_df.index\n",
        "    return parameters_df\n",
        "\n",
        "def create_data(parameters_df: pd.DataFrame, num_records: int = 100,\n",
        "                min_amount: float = 100.0, max_amount: float = 10000.0) -> pd.DataFrame: # Added amount range\n",
        "    records = []\n",
        "    deposit_types = parameters_df['Deposit_Type'].tolist()\n",
        "    max_lifetimes = parameters_df.set_index('Deposit_Type')['Lifetime'].to_dict()\n",
        "    costs = parameters_df.set_index('Deposit_Type')['Cost'].to_dict() # Cost per unit\n",
        "    type_indices = parameters_df.set_index('Deposit_Type')['Type_Index'].to_dict()\n",
        "\n",
        "    for _ in range(num_records):\n",
        "        deposit_type = random.choice(deposit_types)\n",
        "        max_lifetime = max_lifetimes[deposit_type]\n",
        "        cost_per_unit = costs[deposit_type] # Get cost per unit\n",
        "        type_index = type_indices[deposit_type]\n",
        "        current_lifetime = random.uniform(0, max_lifetime * 0.8) # Start with some life already passed\n",
        "        amount = random.uniform(min_amount, max_amount) # Assign random amount\n",
        "\n",
        "        records.append({\n",
        "            'Deposit_Type': deposit_type,\n",
        "            'Current_Lifetime': current_lifetime,\n",
        "            'Cost': cost_per_unit, # Store cost per unit\n",
        "            'Amount': amount,      # Store the amount\n",
        "            'Type_Index': type_index\n",
        "        })\n",
        "    df = pd.DataFrame(records)\n",
        "    # Ensure correct dtypes from the start\n",
        "    df['Current_Lifetime'] = pd.to_numeric(df['Current_Lifetime'], errors='coerce')\n",
        "    df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "    df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')\n",
        "    df['Type_Index'] = pd.to_numeric(df['Type_Index'], errors='coerce').astype(int)\n",
        "    return df\n",
        "\n",
        "\n",
        "# --- Environment Definition (Modified for RNN State) ---\n",
        "\n",
        "class DepositManagementEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
        "\n",
        "    def __init__(self, initial_df: pd.DataFrame, parameters_df: pd.DataFrame,\n",
        "                 embedding_dim: int, # <<< New: Dimension of the RNN embedding\n",
        "                 max_steps: int = 100,\n",
        "                 cost_weight: float = 0.5,         # Weight for cost penalty\n",
        "                 diversity_weight: float = 0.5,      # Weight for diversity reward\n",
        "                 roll_off_penalty_weight: float = 1.0, # Weight for roll-off penalty\n",
        "                 render_mode: Optional[str] = None,\n",
        "                 # Buckets are now only needed for INFO calculation, not observation\n",
        "                 remaining_lifetime_buckets: List[float] = [0, 3, 6, 12, 24, 48, float('inf')]\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        if 'Amount' not in initial_df.columns:\n",
        "            raise ValueError(\"Initial DataFrame must include an 'Amount' column.\")\n",
        "\n",
        "        self.initial_df = initial_df.copy()\n",
        "        self.parameters_df = parameters_df\n",
        "        self.n_deposit_types = len(parameters_df)\n",
        "        self.max_steps = max_steps\n",
        "        self.cost_weight = cost_weight\n",
        "        self.diversity_weight = diversity_weight\n",
        "        self.roll_off_penalty_weight = roll_off_penalty_weight\n",
        "        self.render_mode = render_mode\n",
        "        self.embedding_dim = embedding_dim # <<< Store embedding dim\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "\n",
        "        # --- Features for RNN ---\n",
        "        # Define the features that will be fed into the RNN for each deposit\n",
        "        # We'll use: Current_Lifetime, Cost (per unit), Amount, Type_Index (maybe one-hot encoded later)\n",
        "        self.rnn_features = ['Current_Lifetime', 'Cost', 'Amount', 'Type_Index']\n",
        "        self.num_rnn_features = len(self.rnn_features)\n",
        "\n",
        "        # --- Normalization ---\n",
        "        # Fit scalers on initial data (or potentially a larger sample)\n",
        "        # We'll apply scaling within the agent's state encoder for simplicity,\n",
        "        # but ideally, scaling parameters would be fixed based on expected ranges.\n",
        "        # For simplicity here, we won't pre-fit scalers in the env.\n",
        "\n",
        "        # Bucket setup (only for info calculation)\n",
        "        self.bucket_boundaries = sorted(list(set(remaining_lifetime_buckets)))\n",
        "        if self.bucket_boundaries[0] != 0: self.bucket_boundaries.insert(0, 0)\n",
        "        if self.bucket_boundaries[-1] != float('inf'): self.bucket_boundaries.append(float('inf'))\n",
        "        self.num_buckets = len(self.bucket_boundaries) - 1\n",
        "        self.bucket_labels = [f\"{self.bucket_boundaries[i]}-{self.bucket_boundaries[i+1]}\" for i in range(self.num_buckets)]\n",
        "\n",
        "        self.max_lifetime_map = self.parameters_df.set_index('Deposit_Type')['Lifetime'].to_dict()\n",
        "        self.type_index_to_name = self.parameters_df.set_index('Type_Index')['Deposit_Type'].to_dict()\n",
        "        self.name_to_type_index = self.parameters_df.set_index('Deposit_Type')['Type_Index'].to_dict()\n",
        "        self.max_possible_cost = self.parameters_df['Cost'].max() if not self.parameters_df.empty else 0\n",
        "        self.max_entropy = np.log(self.n_deposit_types) if self.n_deposit_types > 1 else 1.0\n",
        "\n",
        "        # Action space remains the same (proportions)\n",
        "        self.action_space = spaces.Box(\n",
        "            low=0, high=1, shape=(self.n_deposit_types,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Observation space: Now represents the fixed-size embedding from the RNN\n",
        "        # The actual data processed by the agent is the raw portfolio sequence.\n",
        "        # We return a placeholder observation matching this shape.\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(self.embedding_dim,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.current_df = pd.DataFrame()\n",
        "        self.current_step = 0\n",
        "        self._info_needs_recalc = True\n",
        "        self.np_random, _ = gym.utils.seeding.np_random()\n",
        "\n",
        "\n",
        "    def _calculate_shannon_entropy(self, proportions: np.ndarray) -> float:\n",
        "        # (No changes needed)\n",
        "        if not isinstance(proportions, np.ndarray): proportions = np.array(proportions)\n",
        "        prop_sum = proportions.sum()\n",
        "        if prop_sum < 1e-9 or len(proportions) == 0: return 0.0\n",
        "        proportions_normalized = proportions / prop_sum\n",
        "        proportions_clipped = np.clip(proportions_normalized, 1e-9, 1.0)\n",
        "        entropy = -np.sum(proportions_clipped * np.log(proportions_clipped))\n",
        "        return entropy\n",
        "\n",
        "    def _prepare_info_data(self) -> Tuple[pd.DataFrame, float, bool]:\n",
        "        \"\"\"Prepares data primarily for the _get_info method (bucketing).\"\"\"\n",
        "        # This logic is similar to the old _prepare_observation_and_info_data,\n",
        "        # but focused only on what's needed for the human-readable info dict.\n",
        "        total_amount = self.current_df['Amount'].sum() if not self.current_df.empty else 0.0\n",
        "        is_empty = total_amount < 1e-9\n",
        "\n",
        "        if is_empty:\n",
        "            temp_df = pd.DataFrame(columns=['Deposit_Type', 'Current_Lifetime', 'Cost', 'Amount', 'Type_Index', 'Remaining_Lifetime', 'Bucket_Index'])\n",
        "            return temp_df, total_amount, True\n",
        "\n",
        "        # Ensure required columns for info calculation are present\n",
        "        required_cols = ['Deposit_Type', 'Current_Lifetime', 'Cost', 'Amount', 'Type_Index']\n",
        "        if not all(col in self.current_df.columns for col in required_cols):\n",
        "            print(f\"CRITICAL WARNING: Required columns missing for info: {required_cols}. Returning empty temp_df.\")\n",
        "            temp_df = pd.DataFrame(columns=required_cols + ['Remaining_Lifetime', 'Bucket_Index'])\n",
        "            return temp_df, 0.0, True\n",
        "\n",
        "        temp_df = self.current_df[required_cols].copy()\n",
        "\n",
        "        # Calculate Remaining Lifetime\n",
        "        try:\n",
        "            max_lifetimes = temp_df['Deposit_Type'].map(self.max_lifetime_map).fillna(temp_df['Current_Lifetime'])\n",
        "            remaining_lifetime_series = max_lifetimes - temp_df['Current_Lifetime']\n",
        "            temp_df['Remaining_Lifetime'] = remaining_lifetime_series.clip(lower=0)\n",
        "            temp_df['Remaining_Lifetime'] = pd.to_numeric(temp_df['Remaining_Lifetime'], errors='coerce').fillna(0)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR calculating remaining lifetime for info: {e}\")\n",
        "            temp_df['Remaining_Lifetime'] = 0.0\n",
        "\n",
        "        # Assign Buckets\n",
        "        try:\n",
        "            bucket_indices = pd.cut(\n",
        "                temp_df['Remaining_Lifetime'],\n",
        "                bins=self.bucket_boundaries, labels=False, right=False, include_lowest=True\n",
        "            )\n",
        "            temp_df['Bucket_Index'] = bucket_indices.fillna(self.num_buckets - 1).astype(int) # Fill NaNs and ensure int\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR calculating bucket indices for info: {e}\")\n",
        "            temp_df['Bucket_Index'] = self.num_buckets - 1 # Assign to last bucket on error\n",
        "\n",
        "        self._info_needs_recalc = False\n",
        "        return temp_df, total_amount, False\n",
        "\n",
        "    def _get_portfolio_state_for_rnn(self) -> Optional[np.ndarray]:\n",
        "        \"\"\"Extracts and potentially pre-processes features for the RNN.\"\"\"\n",
        "        if self.current_df.empty:\n",
        "            return None # Or return an array of shape (0, num_rnn_features)\n",
        "\n",
        "        # Select features\n",
        "        try:\n",
        "            state_data = self.current_df[self.rnn_features].copy()\n",
        "            # Basic NaN check/fill before converting to numpy\n",
        "            for col in self.rnn_features:\n",
        "                if state_data[col].isnull().any():\n",
        "                     # Simple fill with 0, more sophisticated imputation might be needed\n",
        "                    state_data[col] = state_data[col].fillna(0)\n",
        "\n",
        "            # Convert to numpy array\n",
        "            state_array = state_data.values.astype(np.float32)\n",
        "            return state_array\n",
        "        except KeyError as e:\n",
        "            print(f\"ERROR: Missing feature column for RNN: {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR preparing state for RNN: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\" Returns a placeholder observation matching the embedding dimension.\n",
        "            The actual state information is passed via the info dict. \"\"\"\n",
        "        # This is now just a formality to comply with the Gym API expecting a Box observation.\n",
        "        # The agent will use the raw data from the info dict.\n",
        "        return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
        "\n",
        "\n",
        "    def _get_info(self) -> Dict[str, Any]:\n",
        "        # Calculates diagnostic information (uses bucketing logic, but NOT for observation)\n",
        "        if not self._info_needs_recalc and hasattr(self, 'cached_info'):\n",
        "            return self.cached_info\n",
        "\n",
        "        # Prepare data needed for bucketing and stats\n",
        "        temp_info_df, total_amount, is_empty = self._prepare_info_data()\n",
        "\n",
        "        # Basic info: Total Amount\n",
        "        info = {\"portfolio_total_amount\": total_amount}\n",
        "\n",
        "        # --- Add the raw portfolio state needed by the agent ---\n",
        "        # This is the crucial part for the RNN agent\n",
        "        info[\"raw_portfolio_state\"] = self._get_portfolio_state_for_rnn() # Get the numpy array of features\n",
        "\n",
        "        # --- Calculate Overall Amount Proportions and Weighted Cost (for logging/rendering) ---\n",
        "        overall_proportions = np.zeros(self.n_deposit_types, dtype=np.float32)\n",
        "        overall_weighted_avg_cost = 0.0\n",
        "        overall_simple_avg_cost = 0.0\n",
        "\n",
        "        if not is_empty:\n",
        "            if 'Type_Index' in temp_info_df.columns and 'Amount' in temp_info_df.columns:\n",
        "                type_total_amounts = temp_info_df.groupby('Type_Index')['Amount'].sum()\n",
        "                for type_idx, amount_sum in type_total_amounts.items():\n",
        "                    if pd.isna(type_idx): continue\n",
        "                    type_idx_int = int(type_idx)\n",
        "                    if 0 <= type_idx_int < self.n_deposit_types:\n",
        "                        overall_proportions[type_idx_int] = amount_sum / total_amount\n",
        "\n",
        "            if 'Cost' in temp_info_df.columns and 'Amount' in temp_info_df.columns:\n",
        "                total_cost = (temp_info_df['Cost'] * temp_info_df['Amount']).sum()\n",
        "                overall_weighted_avg_cost = total_cost / total_amount if total_amount > 0 else 0.0\n",
        "                overall_simple_avg_cost = temp_info_df['Cost'].mean()\n",
        "\n",
        "        info[\"overall_proportions\"] = overall_proportions.copy()\n",
        "        info[\"overall_diversity_score\"] = self._calculate_shannon_entropy(overall_proportions)\n",
        "        info[\"overall_weighted_avg_cost\"] = overall_weighted_avg_cost\n",
        "        info[\"overall_simple_avg_cost\"] = overall_simple_avg_cost\n",
        "\n",
        "\n",
        "        # --- Detailed Bucketed Summary (for logging/rendering) ---\n",
        "        bucketed_info = {}\n",
        "        if not is_empty:\n",
        "            for i in range(self.num_buckets):\n",
        "                bucket_df = temp_info_df[temp_info_df['Bucket_Index'] == i]\n",
        "                amount_in_bucket = bucket_df['Amount'].sum()\n",
        "                bucket_label = self.bucket_labels[i]\n",
        "                bucket_stats = { \"amount\": amount_in_bucket, \"proportion\": 0.0, \"avg_unit_cost\": 0.0,\n",
        "                                 \"weighted_avg_cost\": 0.0, \"std_unit_cost\": 0.0, \"diversity\": 0.0,\n",
        "                                 \"type_props\": [0.0] * self.n_deposit_types }\n",
        "                if amount_in_bucket > 0 and total_amount > 0:\n",
        "                     bucket_stats[\"proportion\"] = amount_in_bucket / total_amount\n",
        "                     if 'Cost' in bucket_df.columns:\n",
        "                         costs_in_bucket = bucket_df['Cost']\n",
        "                         bucket_stats[\"avg_unit_cost\"] = costs_in_bucket.mean()\n",
        "                         total_cost_bucket = (bucket_df['Cost'] * bucket_df['Amount']).sum()\n",
        "                         bucket_stats[\"weighted_avg_cost\"] = total_cost_bucket / amount_in_bucket\n",
        "                         if len(bucket_df) > 1:\n",
        "                             std_val = costs_in_bucket.std()\n",
        "                             bucket_stats[\"std_unit_cost\"] = 0.0 if pd.isna(std_val) else std_val\n",
        "                     if 'Type_Index' in bucket_df.columns:\n",
        "                         props_bucket_vec = np.zeros(self.n_deposit_types, dtype=np.float32)\n",
        "                         type_amounts_bucket = bucket_df.groupby('Type_Index')['Amount'].sum()\n",
        "                         for type_idx, amount_sum in type_amounts_bucket.items():\n",
        "                            if pd.isna(type_idx): continue\n",
        "                            type_idx_int = int(type_idx)\n",
        "                            if 0 <= type_idx_int < self.n_deposit_types:\n",
        "                                props_bucket_vec[type_idx_int] = amount_sum / amount_in_bucket\n",
        "                         bucket_stats[\"type_props\"] = props_bucket_vec.tolist()\n",
        "                         bucket_stats[\"diversity\"] = self._calculate_shannon_entropy(props_bucket_vec)\n",
        "                bucketed_info[bucket_label] = bucket_stats\n",
        "        else: # Empty portfolio case\n",
        "             bucketed_info = {label: {\"amount\": 0.0, \"proportion\": 0.0, \"avg_unit_cost\": 0.0,\n",
        "                                     \"weighted_avg_cost\": 0.0, \"std_unit_cost\": 0.0, \"diversity\": 0.0,\n",
        "                                     \"type_props\": [0.0]*self.n_deposit_types}\n",
        "                              for label in self.bucket_labels}\n",
        "\n",
        "        info[\"bucketed_summary\"] = bucketed_info\n",
        "        self.cached_info = info\n",
        "        # self._info_needs_recalc is set to False in _prepare_info_data\n",
        "        return info\n",
        "\n",
        "\n",
        "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = 0\n",
        "        self.current_df = self.initial_df.copy()\n",
        "\n",
        "        # --- Data Validation and Correction (same as before) ---\n",
        "        if 'Amount' not in self.current_df.columns:\n",
        "             print(\"CRITICAL WARNING: 'Amount' column missing...\")\n",
        "             self.current_df['Amount'] = 1.0\n",
        "        else:\n",
        "             self.current_df['Amount'] = pd.to_numeric(self.current_df['Amount'], errors='coerce').fillna(0.0)\n",
        "             self.current_df['Amount'] = self.current_df['Amount'].clip(lower=0.0)\n",
        "\n",
        "        if 'Cost' not in self.current_df.columns and 'Deposit_Type' in self.current_df.columns:\n",
        "             cost_map = self.parameters_df.set_index('Deposit_Type')['Cost'].to_dict()\n",
        "             self.current_df['Cost'] = self.current_df['Deposit_Type'].map(cost_map)\n",
        "             # print(\"Info: Added missing 'Cost' (unit cost) column during reset.\")\n",
        "        elif 'Cost' in self.current_df.columns:\n",
        "              self.current_df['Cost'] = pd.to_numeric(self.current_df['Cost'], errors='coerce').fillna(0.0) # Fill NaNs\n",
        "\n",
        "        if 'Type_Index' not in self.current_df.columns and 'Deposit_Type' in self.current_df.columns:\n",
        "             self.current_df['Type_Index'] = self.current_df['Deposit_Type'].map(self.name_to_type_index)\n",
        "             # print(\"Info: Added missing 'Type_Index' column during reset.\")\n",
        "        elif 'Type_Index' in self.current_df.columns:\n",
        "              # Ensure Type_Index is integer, handle potential NaNs from map/coerce\n",
        "              self.current_df['Type_Index'] = pd.to_numeric(self.current_df['Type_Index'], errors='coerce').fillna(-1).astype(int) # Fill NaN with -1 or similar\n",
        "\n",
        "        if 'Deposit_Type' not in self.current_df.columns or 'Current_Lifetime' not in self.current_df.columns:\n",
        "             print(\"CRITICAL WARNING: 'Deposit_Type' or 'Current_Lifetime' missing after reset.\")\n",
        "             self.current_df = pd.DataFrame(columns=['Deposit_Type', 'Current_Lifetime', 'Cost', 'Amount', 'Type_Index'])\n",
        "        else:\n",
        "             self.current_df['Current_Lifetime'] = pd.to_numeric(self.current_df['Current_Lifetime'], errors='coerce').fillna(0.0)\n",
        "\n",
        "        # Remove rows with zero/negative amount or invalid type index\n",
        "        initial_rows = len(self.current_df)\n",
        "        self.current_df = self.current_df[self.current_df['Amount'] > 1e-9]\n",
        "        self.current_df = self.current_df[self.current_df['Type_Index'] >= 0] # Remove rows with invalid type index\n",
        "        if len(self.current_df) < initial_rows:\n",
        "             print(f\"Info: Removed {initial_rows - len(self.current_df)} rows with non-positive amount or invalid type index during reset.\")\n",
        "\n",
        "\n",
        "        # --- Reset Internal State ---\n",
        "        self._info_needs_recalc = True\n",
        "        if hasattr(self, 'cached_info'): del self.cached_info\n",
        "\n",
        "        # --- Get placeholder observation and full info dict ---\n",
        "        observation = self._get_observation() # Returns placeholder\n",
        "        info = self._get_info() # Contains 'raw_portfolio_state'\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame(current_info=info)\n",
        "\n",
        "        # Ensure observation matches space even if placeholder logic failed\n",
        "        if observation.shape != self.observation_space.shape:\n",
        "            observation = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "\n",
        "    def step(self, action_proportions: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:\n",
        "        # --- Mark info/obs as stale ---\n",
        "        self._info_needs_recalc = True\n",
        "        if hasattr(self, 'cached_info'): del self.cached_info\n",
        "\n",
        "        self.current_step += 1\n",
        "        amount_expired = 0.0\n",
        "        total_cost_of_new_deposits = 0.0\n",
        "        amount_added = 0.0\n",
        "        new_deposits_list = []\n",
        "\n",
        "        # Store amount before step\n",
        "        amount_before_step = self.current_df['Amount'].sum() if not self.current_df.empty else 0.0\n",
        "\n",
        "        # --- 1. Increment Current Lifetime (same logic) ---\n",
        "        if not self.current_df.empty and 'Current_Lifetime' in self.current_df.columns:\n",
        "             self.current_df['Current_Lifetime'] = pd.to_numeric(self.current_df['Current_Lifetime'], errors='coerce').fillna(0) + 1\n",
        "\n",
        "        # --- 2. Check for Expired Deposits (same logic) ---\n",
        "        df_after_removal = self.current_df # Start with current df\n",
        "        if not self.current_df.empty and 'Current_Lifetime' in self.current_df.columns and 'Deposit_Type' in self.current_df.columns:\n",
        "            try:\n",
        "                max_lifetimes = self.current_df['Deposit_Type'].map(self.max_lifetime_map)\n",
        "                expired_mask = (self.current_df['Current_Lifetime'] >= max_lifetimes) | max_lifetimes.isnull()\n",
        "                if expired_mask.any():\n",
        "                    amount_expired = self.current_df.loc[expired_mask, 'Amount'].sum()\n",
        "                    df_after_removal = self.current_df.loc[~expired_mask].copy() # Keep non-expired\n",
        "                else:\n",
        "                    amount_expired = 0.0\n",
        "                    # df_after_removal remains self.current_df (no copy needed yet)\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR during expiration check (Step {self.current_step}): {e}. Assuming none expired.\")\n",
        "                amount_expired = 0.0\n",
        "                # df_after_removal remains self.current_df\n",
        "        else:\n",
        "             amount_expired = 0.0\n",
        "\n",
        "        # --- 3. Replace Expired Amount (same logic) ---\n",
        "        if amount_expired > 1e-9:\n",
        "            props = np.array(action_proportions, dtype=np.float64)\n",
        "            props = np.clip(props, 1e-9, None)\n",
        "            prop_sum = np.sum(props)\n",
        "            if prop_sum > 1e-9:\n",
        "                 normalized_proportions = props / prop_sum\n",
        "            else:\n",
        "                 normalized_proportions = np.ones(self.n_deposit_types, dtype=np.float64) / self.n_deposit_types\n",
        "            normalized_proportions /= normalized_proportions.sum()\n",
        "\n",
        "            target_amounts = amount_expired * normalized_proportions\n",
        "            if 'Type_Index' not in self.parameters_df.columns: self.parameters_df['Type_Index'] = self.parameters_df.index\n",
        "\n",
        "            for type_index, target_amount in enumerate(target_amounts):\n",
        "                 if target_amount > 1e-9 and 0 <= type_index < len(self.parameters_df):\n",
        "                     try:\n",
        "                         params = self.parameters_df.iloc[type_index]\n",
        "                         cost_for_this_deposit = target_amount * float(params['Cost'])\n",
        "                         total_cost_of_new_deposits += cost_for_this_deposit\n",
        "                         amount_added += target_amount\n",
        "                         new_deposits_list.append({\n",
        "                             'Deposit_Type': params['Deposit_Type'],\n",
        "                             'Current_Lifetime': 0.0,\n",
        "                             'Cost': float(params['Cost']),\n",
        "                             'Amount': target_amount,\n",
        "                             'Type_Index': int(params['Type_Index'])\n",
        "                         })\n",
        "                     except (KeyError, IndexError, ValueError) as e:\n",
        "                         print(f\"Error Step {self.current_step}: creating new deposit for type_index {type_index}: {e}\")\n",
        "\n",
        "        # --- 4. Add New Deposits (same logic, careful with concat) ---\n",
        "        if new_deposits_list:\n",
        "            new_deposits_df = pd.DataFrame(new_deposits_list)\n",
        "             # Ensure dtypes match before concatenating (important!)\n",
        "            for col in df_after_removal.columns:\n",
        "                if col in new_deposits_df.columns and new_deposits_df[col].dtype != df_after_removal[col].dtype:\n",
        "                    try:\n",
        "                        # Use nullable Int for Type_Index to handle potential future issues\n",
        "                        if col == 'Type_Index':\n",
        "                            new_deposits_df[col] = pd.to_numeric(new_deposits_df[col], errors='coerce').astype('Int64')\n",
        "                            # Ensure the original df column is also compatible (might need adjustment in reset too)\n",
        "                            if df_after_removal[col].dtype != 'Int64':\n",
        "                                df_after_removal[col] = pd.to_numeric(df_after_removal[col], errors='coerce').astype('Int64')\n",
        "                        else:\n",
        "                            new_deposits_df[col] = new_deposits_df[col].astype(df_after_removal[col].dtype)\n",
        "                    except Exception as e:\n",
        "                         print(f\"Warning (Step {self.current_step}): Could not coerce column '{col}' in new deposits. Error: {e}\")\n",
        "\n",
        "            # Robust concat\n",
        "            try:\n",
        "                # Align columns, important if new_deposits_df is empty or has different columns initially\n",
        "                all_cols = list(set(df_after_removal.columns).union(set(new_deposits_df.columns)))\n",
        "                df_after_removal = df_after_removal.reindex(columns=all_cols)\n",
        "                new_deposits_df = new_deposits_df.reindex(columns=all_cols)\n",
        "                self.current_df = pd.concat([df_after_removal, new_deposits_df], ignore_index=True)\n",
        "            except Exception as concat_err:\n",
        "                 print(f\"Error Step {self.current_step}: Concat failed: {concat_err}. Portfolio state might be corrupted.\")\n",
        "                 self.current_df = df_after_removal # Fallback\n",
        "        else: # No new deposits added\n",
        "             self.current_df = df_after_removal # Just the dataframe after removals\n",
        "\n",
        "        # --- 5. Ensure Correct Dtypes After Potential Concat (same logic) ---\n",
        "        # Re-apply type coercion after potential concat, using nullable Int for Type_Index\n",
        "        for col, target_type in [('Amount', float), ('Cost', float), ('Current_Lifetime', float), ('Type_Index', 'Int64')]:\n",
        "            if col in self.current_df.columns:\n",
        "                try:\n",
        "                    if target_type == 'Int64':\n",
        "                         self.current_df[col] = pd.to_numeric(self.current_df[col], errors='coerce').astype('Int64')\n",
        "                    else:\n",
        "                         self.current_df[col] = pd.to_numeric(self.current_df[col], errors='coerce')\n",
        "                    # Check for NaNs introduced if coercion failed for numeric types\n",
        "                    if target_type == float and self.current_df[col].isnull().any():\n",
        "                        # print(f\"Warning (Step {self.current_step}): NaNs detected in '{col}' after coercion. Filling with 0.\")\n",
        "                        self.current_df[col] = self.current_df[col].fillna(0.0)\n",
        "                except Exception as e:\n",
        "                     print(f\"Warning (Step {self.current_step}): Failed dtype conversion for '{col}' after concat. Error: {e}\")\n",
        "\n",
        "\n",
        "        # --- 6. Calculate Reward Components (same logic) ---\n",
        "        avg_cost_per_unit_new = (total_cost_of_new_deposits / amount_added) if amount_added > 1e-9 else 0.0\n",
        "        normalized_cost_penalty = max(0.0, (avg_cost_per_unit_new / self.max_possible_cost) if self.max_possible_cost > 0 else 0.0)\n",
        "        cost_reward_component = -normalized_cost_penalty * self.cost_weight\n",
        "\n",
        "        diversity_score = 0.0\n",
        "        current_portfolio_amount = self.current_df['Amount'].sum() if not self.current_df.empty else 0.0\n",
        "        if current_portfolio_amount > 1e-9 and 'Type_Index' in self.current_df.columns:\n",
        "             # Check for nullable Int type before groupby\n",
        "             if pd.api.types.is_integer_dtype(self.current_df['Type_Index']) or pd.api.types.is_float_dtype(self.current_df['Type_Index']): # Allow float if conversion failed but still numeric\n",
        "                # Drop rows with NA Type_Index before grouping if using nullable Int\n",
        "                valid_types_df = self.current_df.dropna(subset=['Type_Index'])\n",
        "                if not valid_types_df.empty:\n",
        "                    final_type_amounts = valid_types_df.groupby('Type_Index')['Amount'].sum()\n",
        "                    overall_proportions_final = np.zeros(self.n_deposit_types, dtype=np.float32)\n",
        "                    for type_idx, amount_sum in final_type_amounts.items():\n",
        "                         # type_idx should already be non-NA here\n",
        "                         type_idx_int = int(type_idx)\n",
        "                         if 0 <= type_idx_int < self.n_deposit_types:\n",
        "                             overall_proportions_final[type_idx_int] = amount_sum / current_portfolio_amount\n",
        "                    diversity_score = self._calculate_shannon_entropy(overall_proportions_final)\n",
        "\n",
        "        normalized_diversity_reward = max(0.0, (diversity_score / self.max_entropy) if self.max_entropy > 0 else 0.0)\n",
        "        diversity_reward_component = normalized_diversity_reward * self.diversity_weight\n",
        "\n",
        "        roll_off_proportion = (amount_expired / amount_before_step) if amount_before_step > 1e-9 else 0.0\n",
        "        roll_off_penalty_component = -roll_off_proportion * self.roll_off_penalty_weight\n",
        "\n",
        "        # --- 7. Final Reward Calculation (same logic) ---\n",
        "        reward = cost_reward_component + diversity_reward_component + roll_off_penalty_component\n",
        "        if np.isnan(reward) or np.isinf(reward):\n",
        "            print(f\"WARNING: NaN/Inf reward detected (Step {self.current_step}). Applying penalty.\")\n",
        "            reward = -10.0\n",
        "\n",
        "        # --- 8. Check Termination/Truncation (same logic) ---\n",
        "        terminated = (self.current_df['Amount'].sum() < 1e-9) if not self.current_df.empty else True\n",
        "        truncated = self.current_step >= self.max_steps\n",
        "\n",
        "        # --- 9. Get Observation (Placeholder) and Info (includes Raw State) ---\n",
        "        self._info_needs_recalc = True # Ensure info is recalculated\n",
        "        observation = self._get_observation() # Gets placeholder\n",
        "        info = self._get_info() # Gets full info dict including 'raw_portfolio_state'\n",
        "\n",
        "        # Add reward components to info\n",
        "        info[\"cost_reward_component\"] = cost_reward_component\n",
        "        info[\"diversity_reward_component\"] = diversity_reward_component\n",
        "        info[\"roll_off_penalty_component\"] = roll_off_penalty_component\n",
        "        info[\"amount_expired\"] = amount_expired\n",
        "        info[\"amount_added\"] = amount_added\n",
        "        info[\"avg_cost_per_unit_of_added\"] = avg_cost_per_unit_new\n",
        "        info[\"roll_off_proportion\"] = roll_off_proportion\n",
        "\n",
        "        # --- 10. Render (Optional) (same logic) ---\n",
        "        if self.render_mode == \"human\":\n",
        "             self._render_frame(rolled_off_amount=amount_expired,\n",
        "                               avg_new_unit_cost=avg_cost_per_unit_new,\n",
        "                               current_info=info)\n",
        "\n",
        "        # Ensure observation matches space before returning\n",
        "        if observation.shape != self.observation_space.shape:\n",
        "            observation = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
        "        # Ensure raw state exists in info, add placeholder if missing\n",
        "        if \"raw_portfolio_state\" not in info:\n",
        "             info[\"raw_portfolio_state\"] = None # Indicate missing state\n",
        "\n",
        "        return observation, float(reward), bool(terminated), bool(truncated), info\n",
        "\n",
        "    # --- render, _render_frame, close methods (No changes needed) ---\n",
        "    def render(self):\n",
        "        if self.render_mode == \"human\":\n",
        "            current_info = self._get_info()\n",
        "            self._render_frame(current_info=current_info)\n",
        "\n",
        "    def _render_frame(self, rolled_off_amount: Optional[float] = None,\n",
        "                      avg_new_unit_cost: Optional[float] = None,\n",
        "                      current_info: Optional[Dict[str, Any]] = None):\n",
        "        # Renders based on info dict (no change needed as it uses summary stats from info)\n",
        "        if not current_info: current_info = self._get_info()\n",
        "        print(f\"\\n--- Step {self.current_step} ---\")\n",
        "        portfolio_amount = current_info.get('portfolio_total_amount', 'N/A')\n",
        "        print(f\"Total Portfolio Amount: {portfolio_amount:,.2f}\" if isinstance(portfolio_amount, (int, float)) else f\"Total Portfolio Amount: {portfolio_amount}\")\n",
        "        if rolled_off_amount is not None: print(f\"Amount Rolled Off: {rolled_off_amount:,.2f}\")\n",
        "        if avg_new_unit_cost is not None: print(f\"Avg Cost/Unit New: {avg_new_unit_cost:.2f}\")\n",
        "\n",
        "        overall_w_cost = current_info.get('overall_weighted_avg_cost', np.nan)\n",
        "        overall_diversity = current_info.get('overall_diversity_score', np.nan)\n",
        "        is_valid_amount = isinstance(portfolio_amount, (int, float)) and not np.isnan(portfolio_amount)\n",
        "\n",
        "        if is_valid_amount and portfolio_amount > 1e-9:\n",
        "            print(f\"Overall W.Avg Cost: {overall_w_cost:.2f} | Overall Diversity: {overall_diversity:.4f}\")\n",
        "            overall_props = current_info.get('overall_proportions', [])\n",
        "            if len(overall_props) == self.n_deposit_types:\n",
        "                 prop_strs = [f\"{self.type_index_to_name.get(i, f'Idx_{i}')}: {p:.1%}\" for i, p in enumerate(overall_props)]\n",
        "                 print(f\"Overall Props: [{', '.join(prop_strs)}]\")\n",
        "            print(\"\\nBucket Summary (Remaining Lifetime):\")\n",
        "            bucket_summary = current_info.get(\"bucketed_summary\", {})\n",
        "            if isinstance(bucket_summary, dict) and bucket_summary:\n",
        "                print(f\"{'Bucket':<12} | {'Amount':>10} | {'Prop':>6} | {'WAvgC':>7} | {'StdUC':>7} | {'Div':>7} | {'TopType (%)':<15}\")\n",
        "                print(\"-\" * 80)\n",
        "                for label in self.bucket_labels:\n",
        "                    stats = bucket_summary.get(label, {})\n",
        "                    amount, prop, w_avg_cost, std_u_cost, diversity = stats.get('amount',0), stats.get('proportion',0), stats.get('weighted_avg_cost',0), stats.get('std_unit_cost',0), stats.get('diversity',0)\n",
        "                    type_props = stats.get('type_props', [])\n",
        "                    top_type_str = \"N/A\"\n",
        "                    if type_props and amount > 1e-9:\n",
        "                        try: top_type_idx = np.argmax(type_props); top_type_name = self.type_index_to_name.get(top_type_idx, f\"Idx_{top_type_idx}\"); top_type_str = f\"{top_type_name} ({type_props[top_type_idx]*100:.0f}%)\"\n",
        "                        except: top_type_str = \"Error\"\n",
        "                    print(f\"{label:<12} | {amount:>10,.1f} | {prop:>6.1%} | {w_avg_cost:>7.1f} | {std_u_cost:>7.1f} | {diversity:>7.4f} | {top_type_str:<15}\")\n",
        "            else: print(\" (No bucket summary available)\")\n",
        "        elif is_valid_amount and portfolio_amount <= 1e-9: print(\"Portfolio Empty\")\n",
        "        else: print(f\"Portfolio amount invalid/missing: {portfolio_amount}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    def close(self):\n",
        "        if hasattr(self, 'current_df'): del self.current_df\n",
        "        if hasattr(self, 'cached_info'): del self.cached_info\n",
        "        print(\"Environment closed.\")\n",
        "\n",
        "\n",
        "# --- PPO Agent Components (Modified for RNN State Encoder) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class StateEncoder(nn.Module):\n",
        "    \"\"\" RNN (LSTM) based state encoder for processing sequence of deposit features. \"\"\"\n",
        "    def __init__(self, input_dim: int, embedding_dim: int, rnn_hidden_dim: int, num_rnn_layers: int = 1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding_dim = embedding_dim # This is the final output size\n",
        "        self.rnn_hidden_dim = rnn_hidden_dim\n",
        "        self.num_rnn_layers = num_rnn_layers\n",
        "\n",
        "        # --- Normalization Layer (Learnable or Fixed) ---\n",
        "        # Using BatchNorm1d applied across the feature dimension before RNN\n",
        "        # Assumes input shape (Batch, SeqLen, Features) -> needs reshape/permute\n",
        "        # Or apply normalization *before* creating the batch in the agent update\n",
        "        # Let's do basic layer normalization within the encoder for simplicity.\n",
        "        # Note: LayerNorm is typically applied *before* the activation in RNNs,\n",
        "        # but applying it to the input features can also help.\n",
        "        self.input_norm = nn.LayerNorm(input_dim) # Normalize across feature dimension\n",
        "\n",
        "        # --- RNN Layer ---\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=rnn_hidden_dim,\n",
        "            num_layers=num_rnn_layers,\n",
        "            batch_first=True, # Input tensor shape: (batch, seq_len, features)\n",
        "            bidirectional=False # Using a standard forward LSTM\n",
        "        )\n",
        "\n",
        "        # --- Output Layer ---\n",
        "        # Linear layer to map final LSTM hidden state to the desired embedding dimension\n",
        "        self.fc_out = nn.Linear(rnn_hidden_dim, embedding_dim)\n",
        "        # Optional: Add activation like Tanh\n",
        "        # self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x: Union[torch.Tensor, torch.nn.utils.rnn.PackedSequence],\n",
        "                lengths: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Processes a batch of sequences (potentially packed).\n",
        "        Args:\n",
        "            x: Input tensor (Batch, SeqLen, Features) or PackedSequence.\n",
        "            lengths: Original lengths of sequences if x is padded tensor (used for packing).\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (Batch, embedding_dim) representing the state embeddings.\n",
        "        \"\"\"\n",
        "        is_packed = isinstance(x, torch.nn.utils.rnn.PackedSequence)\n",
        "\n",
        "        if not is_packed:\n",
        "            if x.dim() != 3: raise ValueError(f\"Expected 3D tensor (Batch, SeqLen, Features), got {x.shape}\")\n",
        "            if lengths is None: raise ValueError(\"Lengths must be provided for padded sequences\")\n",
        "\n",
        "            # Apply input normalization\n",
        "            # LayerNorm expects (N, *, H_in), operates on last dim (Features)\n",
        "            x_norm = self.input_norm(x)\n",
        "\n",
        "            # Pack sequence for efficient LSTM processing\n",
        "            # Ensure lengths are on the correct device and type\n",
        "            lengths_cpu = lengths.cpu().long() # pack_padded_sequence requires lengths on CPU\n",
        "            packed_input = pack_padded_sequence(x_norm, lengths_cpu, batch_first=True, enforce_sorted=False)\n",
        "        else:\n",
        "            # Assume normalization happened before packing if input is already packed\n",
        "            packed_input = x\n",
        "\n",
        "        # Pass through LSTM\n",
        "        # Output: packed_output (all hidden states), (h_n, c_n) (last hidden/cell states)\n",
        "        packed_output, (h_n, c_n) = self.lstm(packed_input)\n",
        "\n",
        "        # We typically use the last hidden state (h_n) as the sequence representation.\n",
        "        # h_n shape: (num_layers * num_directions, batch, rnn_hidden_dim)\n",
        "        # For uni-directional LSTM, num_directions=1. We want the last layer's hidden state.\n",
        "        # [-1] indexes the last layer.\n",
        "        last_hidden_state = h_n[-1] # Shape: (batch, rnn_hidden_dim)\n",
        "\n",
        "        # Pass through output layer to get final embedding\n",
        "        embedding = self.fc_out(last_hidden_state)\n",
        "        # embedding = self.tanh(embedding) # Optional Tanh activation\n",
        "\n",
        "        return embedding # Shape: (batch, embedding_dim)\n",
        "\n",
        "    def get_default_embedding(self, batch_size: int = 1) -> torch.Tensor:\n",
        "        \"\"\" Returns a zero embedding for empty portfolios. \"\"\"\n",
        "        return torch.zeros((batch_size, self.embedding_dim), device=device, dtype=torch.float32)\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    # Now takes embedding_dim as input instead of state_dim\n",
        "    def __init__(self, embedding_dim: int, action_dim: int, hidden_dim: int = 64):\n",
        "        super().__init__()\n",
        "        if action_dim <= 0: raise ValueError(f\"action_dim must be positive, got {action_dim}\")\n",
        "        self.action_dim = action_dim\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim), nn.ReLU(), # Input is embedding\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim), nn.Softplus() # Softplus ensures alphas > 0\n",
        "        )\n",
        "        self._init_weights() # Keep weight initialization\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.orthogonal_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
        "                if module.bias is not None: nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, embedding: torch.Tensor) -> Dirichlet: # Input is embedding\n",
        "        if embedding.dtype != torch.float32: embedding = embedding.float()\n",
        "        if embedding.dim() == 1: embedding = embedding.unsqueeze(0)\n",
        "\n",
        "        alphas = self.network(embedding) + 1e-6\n",
        "        if alphas.shape[-1] != self.action_dim:\n",
        "            raise ValueError(f\"Alpha dimension mismatch! Expected {self.action_dim}, got {alphas.shape[-1]}. Embedding shape: {embedding.shape}\")\n",
        "        try:\n",
        "            dist = Dirichlet(alphas)\n",
        "        except ValueError as e:\n",
        "            print(f\"ERROR creating Dirichlet distribution: {e}\")\n",
        "            print(f\"Alphas: min={alphas.min():.4f}, max={alphas.max():.4f}, nan={torch.isnan(alphas).any()}, shape={alphas.shape}\")\n",
        "            uniform_alphas = torch.ones_like(alphas)\n",
        "            dist = Dirichlet(uniform_alphas)\n",
        "        return dist\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "     # Now takes embedding_dim as input instead of state_dim\n",
        "    def __init__(self, embedding_dim: int, hidden_dim: int = 64):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim), nn.ReLU(), # Input is embedding\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1) # Output a single value\n",
        "        )\n",
        "        self._init_weights() # Keep weight initialization\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.orthogonal_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
        "                if module.bias is not None: nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, embedding: torch.Tensor) -> torch.Tensor: # Input is embedding\n",
        "        if embedding.dtype != torch.float32: embedding = embedding.float()\n",
        "        if embedding.dim() == 1: embedding = embedding.unsqueeze(0)\n",
        "        return self.network(embedding)\n",
        "\n",
        "\n",
        "class PPOMemory:\n",
        "    # Stores transitions for PPO updates, now stores RAW STATES (sequences)\n",
        "    def __init__(self):\n",
        "        # Store raw portfolio states (e.g., List[np.ndarray] or List[List[Dict]])\n",
        "        # Using List[Optional[np.ndarray]] where None indicates empty portfolio\n",
        "        self.raw_states: List[Optional[np.ndarray]] = []\n",
        "        self.actions: List[np.ndarray] = []\n",
        "        self.log_probs: List[float] = []\n",
        "        self.rewards: List[float] = []\n",
        "        self.values: List[float] = [] # Values correspond to the embedding of the raw_state\n",
        "        self.terminateds: List[bool] = []\n",
        "        self.truncateds: List[bool] = []\n",
        "\n",
        "    def store(self, raw_state: Optional[np.ndarray], action: np.ndarray, log_prob: float,\n",
        "              reward: float, value: float, terminated: bool, truncated: bool):\n",
        "        # Store the raw state (numpy array of features or None)\n",
        "        self.raw_states.append(raw_state.copy() if raw_state is not None else None)\n",
        "        self.actions.append(np.array(action, dtype=np.float32))\n",
        "        self.log_probs.append(float(log_prob))\n",
        "        self.rewards.append(float(reward))\n",
        "        self.values.append(float(value)) # Value is associated with this raw_state\n",
        "        self.terminateds.append(bool(terminated))\n",
        "        self.truncateds.append(bool(truncated))\n",
        "\n",
        "    def clear(self):\n",
        "        self.raw_states.clear(); self.actions.clear(); self.log_probs.clear()\n",
        "        self.rewards.clear(); self.values.clear(); self.terminateds.clear()\n",
        "        self.truncateds.clear()\n",
        "\n",
        "    def get_batch(self) -> Tuple[List[Optional[np.ndarray]], torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # Returns raw states as a list and other data as tensors\n",
        "        if not self.raw_states: raise ValueError(\"Cannot get batch from empty memory.\")\n",
        "\n",
        "        try:\n",
        "            # Keep raw_states as a list (will be handled by agent's update)\n",
        "            batch_raw_states = list(self.raw_states)\n",
        "\n",
        "            actions = torch.tensor(np.array(self.actions), dtype=torch.float32).to(device)\n",
        "            log_probs = torch.tensor(self.log_probs, dtype=torch.float32).to(device)\n",
        "            rewards = torch.tensor(self.rewards, dtype=torch.float32).to(device)\n",
        "            values = torch.tensor(self.values, dtype=torch.float32).to(device) # Values from critic (based on embedding)\n",
        "            terminateds = torch.tensor(self.terminateds, dtype=torch.float32).to(device)\n",
        "            truncateds = torch.tensor(self.truncateds, dtype=torch.float32).to(device)\n",
        "\n",
        "            return batch_raw_states, actions, log_probs, rewards, values, terminateds, truncateds\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting memory batch: {e}\")\n",
        "            raise\n",
        "\n",
        "    def __len__(self) -> int: return len(self.raw_states)\n",
        "\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_rnn_features: int, # Input features per deposit\n",
        "        embedding_dim: int,    # Output dim of StateEncoder, input dim for Actor/Critic\n",
        "        action_dim: int,\n",
        "        rnn_hidden_dim: int,   # Hidden dim of LSTM\n",
        "        actor_critic_hidden_dim: int, # Hidden dim for Actor/Critic MLP\n",
        "        lr_encoder: float,     # Learning rate for StateEncoder\n",
        "        lr_actor: float,\n",
        "        lr_critic: float,\n",
        "        gamma: float, gae_lambda: float, clip_ratio: float,\n",
        "        entropy_coef: float, value_coef: float, max_grad_norm: float,\n",
        "        update_epochs: int, batch_size: int\n",
        "    ):\n",
        "        self.gamma = gamma; self.gae_lambda = gae_lambda; self.clip_ratio = clip_ratio\n",
        "        self.entropy_coef = entropy_coef; self.value_coef = value_coef; self.max_grad_norm = max_grad_norm\n",
        "        self.update_epochs = update_epochs; self.batch_size = batch_size\n",
        "        self.action_dim = action_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_rnn_features = num_rnn_features\n",
        "\n",
        "        # --- Initialize Networks ---\n",
        "        self.encoder = StateEncoder(num_rnn_features, embedding_dim, rnn_hidden_dim).to(device)\n",
        "        self.actor = Actor(embedding_dim, action_dim, actor_critic_hidden_dim).to(device)\n",
        "        self.critic = Critic(embedding_dim, actor_critic_hidden_dim).to(device)\n",
        "\n",
        "        # --- Optimizers ---\n",
        "        # Combine encoder and actor parameters if desired, or keep separate\n",
        "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=lr_encoder, eps=1e-5)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor, eps=1e-5)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic, eps=1e-5)\n",
        "\n",
        "        self.memory = PPOMemory()\n",
        "\n",
        "        # --- Normalization Scaler (Optional but recommended) ---\n",
        "        # Fit this scaler on representative data (e.g., initial deposits or a larger sample)\n",
        "        # For simplicity, we'll use a basic scaler here and apply it during batch processing.\n",
        "        # Note: Using StandardScaler requires fitting. Alternatively, use min-max based on known ranges.\n",
        "        self.scaler = StandardScaler() # Needs to be fit\n",
        "        self._is_scaler_fit = False\n",
        "\n",
        "    def _fit_scaler(self, sample_data_list: List[np.ndarray]):\n",
        "        \"\"\" Fits the StandardScaler on a sample of deposit data. \"\"\"\n",
        "        # Concatenate non-empty arrays from the list\n",
        "        valid_data = [data for data in sample_data_list if data is not None and data.shape[0] > 0]\n",
        "        if not valid_data:\n",
        "            print(\"Warning: No valid data provided to fit scaler.\")\n",
        "            return\n",
        "        try:\n",
        "            all_features = np.concatenate(valid_data, axis=0)\n",
        "            if all_features.shape[0] > 1 and all_features.shape[1] == self.num_rnn_features:\n",
        "                 print(f\"Fitting scaler on {all_features.shape[0]} samples...\")\n",
        "                 self.scaler.fit(all_features)\n",
        "                 self._is_scaler_fit = True\n",
        "                 print(\"Scaler fit successfully.\")\n",
        "                 print(f\"Scaler Mean: {self.scaler.mean_}\")\n",
        "                 print(f\"Scaler Scale (StdDev): {self.scaler.scale_}\")\n",
        "            else:\n",
        "                 print(f\"Warning: Insufficient or invalid data shape ({all_features.shape}) for scaler fitting.\")\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Error fitting scaler: {e}. Scaler will not be used.\")\n",
        "            self._is_scaler_fit = False\n",
        "        except Exception as e:\n",
        "             print(f\"Unexpected error fitting scaler: {e}. Scaler will not be used.\")\n",
        "             self._is_scaler_fit = False\n",
        "\n",
        "\n",
        "    def _preprocess_raw_state(self, raw_state: Optional[np.ndarray]) -> torch.Tensor:\n",
        "        \"\"\" Converts a single raw state (np.ndarray or None) to a tensor suitable for the encoder. \"\"\"\n",
        "        if raw_state is None or raw_state.shape[0] == 0:\n",
        "            # Handle empty portfolio: Return placeholder or signal for default embedding\n",
        "             return self.encoder.get_default_embedding(batch_size=1) # Use encoder's default\n",
        "\n",
        "        # Ensure float32\n",
        "        state_tensor = torch.tensor(raw_state, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Apply normalization if scaler is fit\n",
        "        if self._is_scaler_fit:\n",
        "            try:\n",
        "                 # Scaler expects (n_samples, n_features)\n",
        "                 scaled_state = self.scaler.transform(raw_state) # Use transform, not fit_transform\n",
        "                 state_tensor = torch.tensor(scaled_state, dtype=torch.float32, device=device)\n",
        "            except Exception as e:\n",
        "                 print(f\"Warning: Scaler transform failed for single state: {e}. Using unscaled data.\")\n",
        "                 state_tensor = torch.tensor(raw_state, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Add batch and sequence dimensions if needed by encoder (already has seq dim from numpy array)\n",
        "        # LSTM expects (batch, seq_len, features)\n",
        "        if state_tensor.dim() == 2: # Should be (seq_len, features)\n",
        "             state_tensor = state_tensor.unsqueeze(0) # Add batch dim -> (1, seq_len, features)\n",
        "\n",
        "        # Calculate lengths (for single instance, length is just seq_len)\n",
        "        lengths = torch.tensor([state_tensor.shape[1]], dtype=torch.long, device=device) # Length tensor on device\n",
        "\n",
        "        # Pass through encoder\n",
        "        embedding = self.encoder(state_tensor, lengths) # Shape (1, embedding_dim)\n",
        "        return embedding\n",
        "\n",
        "\n",
        "    def select_action(self, raw_state: Optional[np.ndarray], deterministic: bool = False) -> Tuple[np.ndarray, float, float]:\n",
        "        # Selects action based on embedding from raw state data\n",
        "        log_prob = 0.0; value = 0.0\n",
        "        action_np = np.ones(self.action_dim, dtype=np.float32) / self.action_dim # Default\n",
        "\n",
        "        try:\n",
        "            # 1. Preprocess raw state and get embedding\n",
        "            # Make sure networks are in eval mode for selection/inference\n",
        "            self.encoder.eval()\n",
        "            self.actor.eval()\n",
        "            self.critic.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                 embedding = self._preprocess_raw_state(raw_state) # Get embedding [1, embedding_dim]\n",
        "\n",
        "                 # 2. Get action distribution and value from embedding\n",
        "                 dist = self.actor(embedding)\n",
        "                 value = self.critic(embedding).squeeze().item() # Get state value\n",
        "\n",
        "                 # 3. Sample or get mean action\n",
        "                 if deterministic:\n",
        "                     action = dist.mean # Mean should be [1, action_dim]\n",
        "                     action = action.squeeze(0) # -> [action_dim]\n",
        "                     action = torch.clamp(action, 1e-6, 1.0 - 1e-6) # Numerical stability\n",
        "                     action = action / action.sum(dim=-1, keepdim=True) # Ensure sum=1\n",
        "                     # Calculate log prob of the mean action\n",
        "                     try: log_prob = dist.log_prob(action.unsqueeze(0)).item()\n",
        "                     except ValueError: log_prob = -float('inf')\n",
        "                 else: # Stochastic action\n",
        "                     try:\n",
        "                         action = dist.sample() # Sample is [1, action_dim]\n",
        "                         action = action.squeeze(0) # -> [action_dim]\n",
        "                         if torch.isnan(action).any() or torch.isinf(action).any(): raise ValueError(\"Sampled NaN/Inf action\")\n",
        "                         action = torch.clamp(action, min=0) # Should be non-negative\n",
        "                         action_sum = action.sum(dim=-1, keepdim=True)\n",
        "                         if action_sum < 1e-6: # Fallback if sum is zero\n",
        "                             action = torch.ones_like(action) / self.action_dim\n",
        "                         else:\n",
        "                            action = action / action_sum # Normalize\n",
        "\n",
        "                         action = torch.clamp(action, 1e-6, 1.0 - 1e-6) # Clamp for log_prob stability\n",
        "                         action = action / action.sum(dim=-1, keepdim=True) # Final normalization\n",
        "                         log_prob = dist.log_prob(action.unsqueeze(0)).item() # Log prob of the sampled action\n",
        "                     except ValueError as e:\n",
        "                         print(f\"Warning: Error during stochastic action sampling/log_prob: {e}. Falling back.\")\n",
        "                         action_np_fallback = np.ones(self.action_dim, dtype=np.float32) / self.action_dim\n",
        "                         action = torch.tensor(action_np_fallback, device=device)\n",
        "                         try: log_prob = dist.log_prob(action.unsqueeze(0)).item()\n",
        "                         except ValueError: log_prob = -float('inf')\n",
        "\n",
        "                 action_np = action.cpu().numpy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR in select_action: {e}. Using default uniform action.\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            action_np = np.ones(self.action_dim, dtype=np.float32) / self.action_dim\n",
        "            log_prob = np.log(1.0 / self.action_dim); value = 0.0\n",
        "\n",
        "        # Final validation\n",
        "        if not isinstance(action_np, np.ndarray): action_np = np.array(action_np, dtype=np.float32)\n",
        "        action_np_sum = action_np.sum()\n",
        "        if not np.isclose(action_np_sum, 1.0, atol=1e-4):\n",
        "             action_np = action_np / (action_np_sum + 1e-9)\n",
        "        action_np = np.clip(action_np, 0, 1)\n",
        "        try: log_prob = float(log_prob)\n",
        "        except: log_prob = -float('inf')\n",
        "        try: value = float(value)\n",
        "        except: value = 0.0\n",
        "\n",
        "        return action_np, log_prob, value\n",
        "\n",
        "    def compute_advantages(self, rewards: torch.Tensor, values: torch.Tensor,\n",
        "                           terminateds: torch.Tensor, truncateds: torch.Tensor,\n",
        "                           next_value: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Computes GAE (no change needed here, operates on rewards and values)\n",
        "        advantages = torch.zeros_like(rewards).to(device)\n",
        "        gae = 0.0\n",
        "        next_value_tensor = torch.tensor([next_value], dtype=torch.float32, device=device)\n",
        "        values_with_next = torch.cat((values, next_value_tensor))\n",
        "\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            mask = 1.0 - terminateds[i] # Don't bootstrap if terminated\n",
        "            delta = rewards[i] + self.gamma * values_with_next[i + 1] * mask - values_with_next[i]\n",
        "            gae = delta + self.gamma * self.gae_lambda * mask * gae\n",
        "            advantages[i] = gae\n",
        "\n",
        "        returns = advantages + values\n",
        "\n",
        "        # Normalize advantages\n",
        "        if len(advantages) > 1:\n",
        "            adv_mean = torch.mean(advantages)\n",
        "            adv_std = torch.std(advantages)\n",
        "            if not torch.isnan(adv_std) and adv_std > 1e-8:\n",
        "                advantages = (advantages - adv_mean) / (adv_std + 1e-8)\n",
        "\n",
        "        return advantages, returns\n",
        "\n",
        "\n",
        "    def update(self) -> Tuple[float, float, float]:\n",
        "        # Performs PPO update using RNN embeddings\n",
        "        if len(self.memory) < self.batch_size:\n",
        "             return 0.0, 0.0, 0.0\n",
        "\n",
        "        # Switch models to training mode\n",
        "        self.encoder.train()\n",
        "        self.actor.train()\n",
        "        self.critic.train()\n",
        "\n",
        "        try:\n",
        "            # Get data - raw_states is List[Optional[np.ndarray]]\n",
        "            raw_states, actions, old_log_probs, rewards, values, terminateds, truncateds = self.memory.get_batch()\n",
        "        except ValueError: return 0.0, 0.0, 0.0\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR retrieving batch from memory: {e}\")\n",
        "            self.memory.clear(); return 0.0, 0.0, 0.0\n",
        "\n",
        "        # --- Estimate value of the state after the last recorded transition ---\n",
        "        next_value = 0.0\n",
        "        if len(self.memory.raw_states) > 0 and not self.memory.terminateds[-1]:\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    # Need to put encoder/critic in eval mode temporarily\n",
        "                    self.encoder.eval()\n",
        "                    self.critic.eval()\n",
        "                    last_raw_state = self.memory.raw_states[-1]\n",
        "                    last_embedding = self._preprocess_raw_state(last_raw_state)\n",
        "                    next_value = self.critic(last_embedding).item()\n",
        "                    # Switch back to train mode\n",
        "                    self.encoder.train()\n",
        "                    self.critic.train()\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Failed to estimate next_value for GAE: {e}\")\n",
        "                next_value = 0.0\n",
        "                # Ensure models are back in train mode if error occurred\n",
        "                self.encoder.train(); self.critic.train(); self.actor.train()\n",
        "\n",
        "\n",
        "        # --- Compute advantages and returns ---\n",
        "        try:\n",
        "            advantages, returns = self.compute_advantages(rewards, values, terminateds, truncateds, next_value)\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR computing advantages/returns: {e}\")\n",
        "            self.memory.clear(); return 0.0, 0.0, 0.0\n",
        "\n",
        "        # --- Perform PPO updates over multiple epochs ---\n",
        "        total_policy_loss, total_value_loss, total_entropy = 0.0, 0.0, 0.0\n",
        "        num_updates = 0\n",
        "\n",
        "        if len(raw_states) == 0:\n",
        "             self.memory.clear(); return 0.0, 0.0, 0.0\n",
        "\n",
        "        indices = np.arange(len(raw_states))\n",
        "\n",
        "        for epoch in range(self.update_epochs):\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for start in range(0, len(raw_states), self.batch_size):\n",
        "                end = start + self.batch_size\n",
        "                if start >= end: continue\n",
        "                batch_indices = indices[start:end]\n",
        "\n",
        "                # --- Prepare Batch for RNN ---\n",
        "                batch_raw_states_list = [raw_states[i] for i in batch_indices]\n",
        "                batch_actions = actions[batch_indices]\n",
        "                batch_old_log_probs = old_log_probs[batch_indices]\n",
        "                batch_advantages = advantages[batch_indices]\n",
        "                batch_returns = returns[batch_indices]\n",
        "\n",
        "                # Convert raw states list to padded tensor batch and get lengths\n",
        "                batch_tensors = []\n",
        "                batch_lengths = []\n",
        "                valid_indices_in_batch = [] # Track indices with non-None states\n",
        "\n",
        "                for i, raw_state in enumerate(batch_raw_states_list):\n",
        "                    if raw_state is not None and raw_state.shape[0] > 0:\n",
        "                         # Normalize if scaler is fit\n",
        "                         if self._is_scaler_fit:\n",
        "                              try:\n",
        "                                   scaled_state = self.scaler.transform(raw_state)\n",
        "                                   state_tensor = torch.tensor(scaled_state, dtype=torch.float32, device=device)\n",
        "                              except Exception: # Fallback if scaling fails\n",
        "                                   state_tensor = torch.tensor(raw_state, dtype=torch.float32, device=device)\n",
        "                         else:\n",
        "                              state_tensor = torch.tensor(raw_state, dtype=torch.float32, device=device)\n",
        "\n",
        "                         batch_tensors.append(state_tensor)\n",
        "                         batch_lengths.append(state_tensor.shape[0])\n",
        "                         valid_indices_in_batch.append(i)\n",
        "                    # else: Handle empty state? We need embeddings for all.\n",
        "\n",
        "                # If all states in batch are empty/invalid, skip batch\n",
        "                if not batch_tensors:\n",
        "                     print(f\"Warning: Skipping batch E{epoch+1} B{start//self.batch_size} - all states were empty/invalid.\")\n",
        "                     continue\n",
        "\n",
        "                # Create padded sequence\n",
        "                # pad_sequence expects list of tensors (SeqLen, Features)\n",
        "                padded_batch = pad_sequence(batch_tensors, batch_first=True, padding_value=0.0)\n",
        "                lengths_tensor = torch.tensor(batch_lengths, dtype=torch.long, device=device) # Lengths on device\n",
        "\n",
        "                # --- Get Embeddings for the Batch ---\n",
        "                try:\n",
        "                    # Encoder forward expects (Batch, SeqLen, Features) and lengths\n",
        "                    batch_embeddings = self.encoder(padded_batch, lengths_tensor) # Shape (batch_size_valid, embedding_dim)\n",
        "                except Exception as e:\n",
        "                     print(f\"Warning: Encoder forward pass failed E{epoch+1} B{start//self.batch_size}. Skip mini-batch. Error: {e}\")\n",
        "                     continue\n",
        "\n",
        "                # --- Filter other batch data to match valid embeddings ---\n",
        "                # Only keep data corresponding to the states that produced valid embeddings\n",
        "                batch_actions = batch_actions[valid_indices_in_batch]\n",
        "                batch_old_log_probs = batch_old_log_probs[valid_indices_in_batch]\n",
        "                batch_advantages = batch_advantages[valid_indices_in_batch]\n",
        "                batch_returns = batch_returns[valid_indices_in_batch]\n",
        "\n",
        "                # --- Calculate current policy distribution, values, and entropy from embeddings ---\n",
        "                try:\n",
        "                    dist = self.actor(batch_embeddings)\n",
        "                    curr_values = self.critic(batch_embeddings).squeeze(-1)\n",
        "                except Exception as e:\n",
        "                     print(f\"Warning: Actor/Critic forward pass failed E{epoch+1} B{start//self.batch_size}. Skip mini-batch. Error: {e}\")\n",
        "                     continue\n",
        "\n",
        "                # --- Calculate log probs, ratio, losses (similar to before, but using embeddings) ---\n",
        "                # Renorm actions slightly for stability\n",
        "                batch_actions_renorm = torch.clamp(batch_actions, 1e-6, 1.0 - 1e-6)\n",
        "                batch_actions_renorm = batch_actions_renorm / (batch_actions_renorm.sum(dim=-1, keepdim=True) + 1e-9)\n",
        "                try: curr_log_probs = dist.log_prob(batch_actions_renorm)\n",
        "                except ValueError as e:\n",
        "                     print(f\"Warning: LogProb calculation failed E{epoch+1} B{start//self.batch_size}. Skip mini-batch. Error: {e}\")\n",
        "                     continue\n",
        "\n",
        "                try: entropy = dist.entropy().mean()\n",
        "                except ValueError: entropy = torch.tensor(0.0, device=device)\n",
        "                if torch.isnan(entropy) or torch.isinf(entropy): entropy = torch.tensor(0.0, device=device)\n",
        "\n",
        "                ratio = torch.exp(curr_log_probs - batch_old_log_probs)\n",
        "\n",
        "                # Check for NaNs/Infs before loss calculation\n",
        "                if torch.isnan(batch_advantages).any() or torch.isinf(batch_advantages).any(): continue\n",
        "                if torch.isnan(batch_returns).any() or torch.isinf(batch_returns).any(): continue\n",
        "                if torch.isnan(curr_values).any() or torch.isinf(curr_values).any(): continue\n",
        "\n",
        "                surr1 = ratio * batch_advantages\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * batch_advantages\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = nn.functional.mse_loss(curr_values, batch_returns)\n",
        "\n",
        "                # --- Combine losses ---\n",
        "                # Encoder loss is driven by both actor and critic using its embeddings\n",
        "                # We backpropagate actor_loss through actor and encoder,\n",
        "                # and critic_loss through critic and encoder.\n",
        "                actor_loss = policy_loss - self.entropy_coef * entropy\n",
        "                critic_loss = self.value_coef * value_loss\n",
        "\n",
        "                # --- Backpropagation ---\n",
        "                # Zero gradients for all optimizers\n",
        "                self.encoder_optimizer.zero_grad()\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                self.critic_optimizer.zero_grad()\n",
        "\n",
        "                # Calculate gradients (retain_graph=True needed if encoder grads come from two losses)\n",
        "                # Backprop critic loss (affects critic and encoder)\n",
        "                if not (torch.isnan(critic_loss) or torch.isinf(critic_loss)):\n",
        "                    # Need to specify retain_graph if actor_loss will also backprop through encoder\n",
        "                    critic_loss.backward(retain_graph=True) # <--- Retain graph\n",
        "\n",
        "                # Backprop actor loss (affects actor and encoder)\n",
        "                if not (torch.isnan(actor_loss) or torch.isinf(actor_loss)):\n",
        "                    actor_loss.backward()\n",
        "                else: # If actor loss is invalid, we still need to clear the graph if critic_loss retained it\n",
        "                    if not (torch.isnan(critic_loss) or torch.isinf(critic_loss)):\n",
        "                       # If only critic_loss was valid, step its optimizer and encoder's\n",
        "                       nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
        "                       nn.utils.clip_grad_norm_(self.encoder.parameters(), self.max_grad_norm) # Clip encoder grads too\n",
        "                       self.critic_optimizer.step()\n",
        "                       self.encoder_optimizer.step() # Step encoder based on critic signal\n",
        "                       continue # Skip actor step\n",
        "\n",
        "\n",
        "                # Clip gradients for all involved networks\n",
        "                nn.utils.clip_grad_norm_(self.encoder.parameters(), self.max_grad_norm)\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
        "\n",
        "                # Step optimizers\n",
        "                self.encoder_optimizer.step()\n",
        "                self.actor_optimizer.step()\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "\n",
        "                # --- Accumulate statistics ---\n",
        "                total_policy_loss += policy_loss.item()\n",
        "                total_value_loss += value_loss.item()\n",
        "                total_entropy += entropy.item()\n",
        "                num_updates += 1\n",
        "\n",
        "\n",
        "        # --- Clear memory after updates ---\n",
        "        self.memory.clear()\n",
        "\n",
        "        if num_updates == 0: return 0.0, 0.0, 0.0\n",
        "        return total_policy_loss / num_updates, total_value_loss / num_updates, total_entropy / num_updates\n",
        "\n",
        "\n",
        "# --- Training Loop (Modified for Raw State Handling) ---\n",
        "def train(env: DepositManagementEnv, agent: PPOAgent, initial_df: pd.DataFrame,\n",
        "          num_episodes: int, update_timestep: int, print_freq: int,\n",
        "          fit_scaler_episodes: int = 10) -> Dict[str, List]: # Add scaler fitting episodes\n",
        "\n",
        "    print(f\"\\nStarting training for {num_episodes} episodes...\")\n",
        "    print(f\"Update trigger: {update_timestep} steps. Batch: {agent.batch_size}, Epochs: {agent.update_epochs}\")\n",
        "    print(f\"Fitting scaler on data from first {fit_scaler_episodes} episodes...\")\n",
        "\n",
        "    results = {\"rewards\": [], \"avg_rewards_plot\": [], \"overall_weighted_costs\": [], \"overall_diversities\": [],\n",
        "               \"episode_lengths\": [], \"policy_losses\": [], \"value_losses\": [], \"entropies\": [],\n",
        "               \"final_portfolio_amounts\": []}\n",
        "    time_step = 0\n",
        "    log_running_reward = 0\n",
        "    log_running_episodes = 0\n",
        "    last_log_update_idx = 0\n",
        "    scaler_fit_data = [] # Collect data for scaler fitting\n",
        "\n",
        "    for episode in range(1, num_episodes + 1):\n",
        "        try:\n",
        "            # Reset returns placeholder obs, and info dict containing raw state\n",
        "            _, info = env.reset() # Ignore placeholder obs\n",
        "            raw_state = info.get(\"raw_portfolio_state\") # Get the actual state for the agent\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR during env.reset() Ep {episode}: {e}. Skipping episode.\")\n",
        "            continue\n",
        "\n",
        "        current_ep_reward = 0.0\n",
        "        ep_weighted_costs, ep_diversities = [], []\n",
        "        terminated, truncated = False, False\n",
        "\n",
        "        # --- Fit Scaler ---\n",
        "        if episode <= fit_scaler_episodes:\n",
        "             if raw_state is not None: scaler_fit_data.append(raw_state)\n",
        "             if episode == fit_scaler_episodes:\n",
        "                 agent._fit_scaler(scaler_fit_data) # Fit the agent's scaler\n",
        "                 scaler_fit_data = [] # Clear the data\n",
        "\n",
        "        for t in range(1, env.max_steps + 1):\n",
        "            time_step += 1\n",
        "            action, log_prob, value = (np.zeros(agent.action_dim), 0.0, 0.0) # Defaults\n",
        "\n",
        "            try:\n",
        "                # --- Select action using raw state ---\n",
        "                action, log_prob, value = agent.select_action(raw_state, deterministic=False)\n",
        "\n",
        "                # Validate action\n",
        "                if not isinstance(action, np.ndarray) or action.shape[0] != agent.action_dim or not np.isclose(action.sum(), 1.0, atol=1e-4):\n",
        "                     print(f\"ERROR: Invalid action Ep {episode} St {t}. Sum: {action.sum()}. Falling back.\")\n",
        "                     action = np.ones(agent.action_dim, dtype=np.float32) / agent.action_dim\n",
        "                     log_prob, value = 0.0, 0.0\n",
        "\n",
        "                # --- Environment Step ---\n",
        "                # Step returns placeholder obs, reward, flags, and info dict with next raw state\n",
        "                _, reward, terminated, truncated, info = env.step(action) # Ignore placeholder obs\n",
        "\n",
        "                # --- Get next raw state for storage ---\n",
        "                next_raw_state = info.get(\"raw_portfolio_state\")\n",
        "\n",
        "                # --- Reward Validation ---\n",
        "                if not isinstance(reward, (float, int)) or np.isnan(reward) or np.isinf(reward):\n",
        "                     print(f\"ERROR: Invalid reward (Ep {episode}, Step {t}): {reward}. Setting to -5.\")\n",
        "                     reward = -5.0\n",
        "\n",
        "                # --- Store Experience (using raw_state, not placeholder) ---\n",
        "                agent.memory.store(raw_state, action, log_prob, reward, value, terminated, truncated)\n",
        "\n",
        "                # --- Collect data for scaler fitting (if still in fitting phase) ---\n",
        "                if episode < fit_scaler_episodes and next_raw_state is not None:\n",
        "                     scaler_fit_data.append(next_raw_state)\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"CRITICAL ERROR during step/action (Ep {episode}, Step {t}): {e}.\")\n",
        "                import traceback; traceback.print_exc()\n",
        "                terminated = True # Terminate episode on critical error\n",
        "\n",
        "\n",
        "            # --- State Transition ---\n",
        "            if not terminated: # Only update state if not terminated by error etc.\n",
        "                raw_state = next_raw_state # IMPORTANT: Update the state for the next iteration\n",
        "                current_ep_reward += reward\n",
        "                ep_weighted_costs.append(info.get('overall_weighted_avg_cost', np.nan))\n",
        "                ep_diversities.append(info.get('overall_diversity_score', np.nan))\n",
        "            else:\n",
        "                 break # Exit step loop if terminated by error\n",
        "\n",
        "            # --- PPO Update Trigger ---\n",
        "            if len(agent.memory) >= update_timestep and len(agent.memory) >= agent.batch_size:\n",
        "                 # Only update if scaler has been fit (or if not using scaler)\n",
        "                 if agent._is_scaler_fit or episode > fit_scaler_episodes: # Allow updates after fitting phase\n",
        "                    try:\n",
        "                        policy_loss, value_loss, entropy = agent.update()\n",
        "                        if not (policy_loss == 0.0 and value_loss == 0.0 and entropy == 0.0):\n",
        "                             if not (np.isnan(policy_loss) or np.isnan(value_loss) or np.isnan(entropy)):\n",
        "                                 results[\"policy_losses\"].append(policy_loss)\n",
        "                                 results[\"value_losses\"].append(value_loss)\n",
        "                                 results[\"entropies\"].append(entropy)\n",
        "                             else:\n",
        "                                 print(f\"Warning: NaN loss/entropy from update (Ep {episode}, Step {t}). Not recorded.\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"CRITICAL ERROR during agent.update (Ep {episode}, Step {t}): {e}\")\n",
        "                        import traceback; traceback.print_exc()\n",
        "                 # else: print(\"Skipping update: Scaler not fit yet.\")\n",
        "\n",
        "\n",
        "            # --- Episode End Condition ---\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        # --- End of Episode Logging ---\n",
        "        results[\"rewards\"].append(current_ep_reward)\n",
        "        results[\"overall_weighted_costs\"].append(np.nanmean(ep_weighted_costs) if ep_weighted_costs else np.nan)\n",
        "        results[\"overall_diversities\"].append(np.nanmean(ep_diversities) if ep_diversities else np.nan)\n",
        "        results[\"episode_lengths\"].append(t)\n",
        "        results[\"final_portfolio_amounts\"].append(info.get('portfolio_total_amount', np.nan))\n",
        "\n",
        "        log_running_reward += current_ep_reward\n",
        "        log_running_episodes += 1\n",
        "\n",
        "        # --- Periodic Logging ---\n",
        "        if episode % print_freq == 0 or episode == num_episodes:\n",
        "             if log_running_episodes > 0:\n",
        "                 avg_reward = log_running_reward / log_running_episodes\n",
        "                 avg_w_cost = np.nanmean(results[\"overall_weighted_costs\"][-log_running_episodes:])\n",
        "                 avg_diversity = np.nanmean(results[\"overall_diversities\"][-log_running_episodes:])\n",
        "                 avg_length = np.nanmean(results[\"episode_lengths\"][-log_running_episodes:])\n",
        "                 avg_final_amount = np.nanmean(results[\"final_portfolio_amounts\"][-log_running_episodes:])\n",
        "\n",
        "                 current_update_idx = len(results[\"policy_losses\"])\n",
        "                 avg_ploss, avg_vloss, avg_ent = np.nan, np.nan, np.nan\n",
        "                 if current_update_idx > last_log_update_idx:\n",
        "                     pl = results[\"policy_losses\"][last_log_update_idx:current_update_idx]; avg_ploss = np.nanmean(pl) if pl else np.nan\n",
        "                     vl = results[\"value_losses\"][last_log_update_idx:current_update_idx]; avg_vloss = np.nanmean(vl) if vl else np.nan\n",
        "                     en = results[\"entropies\"][last_log_update_idx:current_update_idx]; avg_ent = np.nanmean(en) if en else np.nan\n",
        "\n",
        "                 print(f\"Ep {episode}/{num_episodes} | Avg R: {avg_reward:,.1f} | Avg WCost: {avg_w_cost:.2f} | Avg Div: {avg_diversity:.3f} \"\n",
        "                       f\"| Avg Len: {avg_length:.1f} | Avg Fin Amt: {avg_final_amount:,.0f} \"\n",
        "                       f\"| P Loss: {avg_ploss:.3f} | V Loss: {avg_vloss:.3f} | Entropy: {avg_ent:.3f}\")\n",
        "\n",
        "                 results[\"avg_rewards_plot\"].append(avg_reward)\n",
        "                 last_log_update_idx = current_update_idx\n",
        "                 log_running_reward = 0\n",
        "                 log_running_episodes = 0\n",
        "             else: print(f\"Ep {episode}/{num_episodes} | No complete episodes finished.\")\n",
        "\n",
        "    print(\"--- Training Finished ---\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# --- Evaluation (Modified for Raw State Handling) ---\n",
        "def evaluate(env: DepositManagementEnv, agent: PPOAgent, initial_df: pd.DataFrame, parameters_df: pd.DataFrame,\n",
        "             num_episodes: int = 10, render: bool = False) -> Tuple[Dict[str, float], Optional[List[np.ndarray]]]:\n",
        "    print(\"\\n--- Starting Evaluation ---\")\n",
        "    all_rewards = []\n",
        "    final_weighted_costs, final_diversities, final_amounts = [], [], []\n",
        "    proportions_history_first_ep: Optional[List[np.ndarray]] = None\n",
        "\n",
        "    original_render_mode = env.render_mode\n",
        "    if render: env.render_mode = \"human\"\n",
        "    else: env.render_mode = None\n",
        "\n",
        "    # Ensure agent is in eval mode\n",
        "    agent.encoder.eval()\n",
        "    agent.actor.eval()\n",
        "    agent.critic.eval()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        episode_proportions = []\n",
        "        try:\n",
        "            # Reset returns placeholder obs, and info dict containing raw state\n",
        "            _, info = env.reset() # Ignore placeholder obs\n",
        "            raw_state = info.get(\"raw_portfolio_state\") # Get the actual state\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR during eval env.reset() Ep {episode + 1}: {e}. Skipping episode.\")\n",
        "            continue\n",
        "\n",
        "        episode_reward = 0.0\n",
        "        terminated, truncated = False, False\n",
        "        if render: print(f\"\\n{'='*10} Eval Ep {episode + 1} {'='*10}\")\n",
        "        ep_steps = 0\n",
        "\n",
        "        # Record initial proportions\n",
        "        initial_props = info.get('overall_proportions', np.zeros(env.n_deposit_types, dtype=np.float32))\n",
        "        episode_proportions.append(np.array(initial_props).copy())\n",
        "\n",
        "        while not (terminated or truncated):\n",
        "            action = np.ones(agent.action_dim, dtype=np.float32) / agent.action_dim # Default\n",
        "            try:\n",
        "                # --- Select action using raw state (deterministic) ---\n",
        "                action, _, _ = agent.select_action(raw_state, deterministic=True)\n",
        "\n",
        "                # Validate action\n",
        "                if not isinstance(action, np.ndarray) or action.shape[0] != agent.action_dim or not np.isclose(action.sum(), 1.0, atol=1e-4):\n",
        "                     print(f\"ERROR: Invalid deterministic action Eval (Ep {episode + 1}, St {ep_steps}). Sum: {action.sum()}. Falling back.\")\n",
        "                     action = np.ones(agent.action_dim, dtype=np.float32) / agent.action_dim\n",
        "\n",
        "                # --- Environment Step ---\n",
        "                _, reward, terminated, truncated, info = env.step(action) # Ignore placeholder obs\n",
        "\n",
        "                # --- Get next raw state ---\n",
        "                next_raw_state = info.get(\"raw_portfolio_state\")\n",
        "\n",
        "                # --- Reward Validation ---\n",
        "                if not isinstance(reward, (float, int)) or np.isnan(reward) or np.isinf(reward):\n",
        "                     print(f\"ERROR: Invalid reward Eval (Ep {episode + 1}, St {ep_steps}): {reward}. Setting to 0.\")\n",
        "                     reward = 0.0\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"CRITICAL ERROR during eval step (Ep {episode + 1}, St {ep_steps}): {e}.\")\n",
        "                import traceback; traceback.print_exc()\n",
        "                terminated = True\n",
        "\n",
        "            # --- Update State and Record ---\n",
        "            if not terminated:\n",
        "                raw_state = next_raw_state # Update state\n",
        "                episode_reward += reward\n",
        "            ep_steps += 1\n",
        "\n",
        "            # Record amount proportions at each step\n",
        "            step_props = info.get('overall_proportions', np.zeros(env.n_deposit_types, dtype=np.float32))\n",
        "            episode_proportions.append(np.array(step_props).copy())\n",
        "\n",
        "            if terminated or truncated: break\n",
        "\n",
        "        # --- End of Evaluation Episode ---\n",
        "        all_rewards.append(episode_reward)\n",
        "        final_weighted_costs.append(info.get('overall_weighted_avg_cost', np.nan))\n",
        "        final_diversities.append(info.get('overall_diversity_score', np.nan))\n",
        "        final_amounts.append(info.get('portfolio_total_amount', np.nan))\n",
        "\n",
        "        if episode == 0:\n",
        "            proportions_history_first_ep = episode_proportions\n",
        "\n",
        "        if render or (episode == num_episodes - 1):\n",
        "             final_w_cost = info.get('overall_weighted_avg_cost', np.nan)\n",
        "             final_div = info.get('overall_diversity_score', np.nan)\n",
        "             final_amt = info.get('portfolio_total_amount', np.nan)\n",
        "             print(f\"End Eval Ep {episode + 1} (Steps: {ep_steps}): Rwd={episode_reward:,.1f} | WCost={final_w_cost:.2f} | Div={final_div:.3f} | Amt={final_amt:,.0f}\")\n",
        "\n",
        "\n",
        "    # --- Calculate Evaluation Summary Statistics ---\n",
        "    avg_reward = np.nanmean(all_rewards) if all_rewards else np.nan\n",
        "    std_reward = np.nanstd(all_rewards) if all_rewards else np.nan\n",
        "    avg_final_w_cost = np.nanmean(final_weighted_costs) if final_weighted_costs else np.nan\n",
        "    avg_final_diversity = np.nanmean(final_diversities) if final_diversities else np.nan\n",
        "    avg_final_amount = np.nanmean(final_amounts) if final_amounts else np.nan\n",
        "\n",
        "    eval_summary = {\n",
        "        \"avg_reward\": avg_reward, \"std_reward\": std_reward,\n",
        "        \"avg_final_weighted_cost\": avg_final_w_cost,\n",
        "        \"avg_final_diversity\": avg_final_diversity,\n",
        "        \"avg_final_amount\": avg_final_amount\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- Evaluation Summary ({num_episodes} episodes) ---\")\n",
        "    print(f\"Avg Reward: {avg_reward:,.2f} +/- {std_reward:,.2f}\")\n",
        "    print(f\"Avg Final WCost: {avg_final_w_cost:.2f}\")\n",
        "    print(f\"Avg Final Div: {avg_final_diversity:.4f}\")\n",
        "    print(f\"Avg Final Amt: {avg_final_amount:,.0f}\")\n",
        "    print(\"-\" * 40 + \"\\n\")\n",
        "\n",
        "    env.render_mode = original_render_mode # Restore render mode\n",
        "    return eval_summary, proportions_history_first_ep\n",
        "\n",
        "\n",
        "# --- Visualization (No changes needed in plotting functions themselves) ---\n",
        "# plot_results and plot_evaluation_distribution should work as long as the\n",
        "# `results` dictionary and `proportions_history` list are populated correctly.\n",
        "\n",
        "def plot_results(results: Dict[str, List], print_freq: int, parameters_df: pd.DataFrame):\n",
        "    if not results or not any(v for v in results.values() if isinstance(v, list) and v):\n",
        "         print(\"No valid training results to plot.\")\n",
        "         return\n",
        "    num_plots = 6\n",
        "    plt.figure(figsize=(15, 12))\n",
        "\n",
        "    # 1. Avg Reward Plot\n",
        "    plt.subplot(num_plots // 2, 2, 1)\n",
        "    avg_rewards_data = results.get(\"avg_rewards_plot\")\n",
        "    if avg_rewards_data and len(avg_rewards_data) > 0:\n",
        "        avg_reward_episodes = np.arange(1, len(avg_rewards_data) + 1) * print_freq\n",
        "        plt.plot(avg_reward_episodes, avg_rewards_data)\n",
        "        plt.title(f\"Avg Reward per {print_freq} Episodes\"); plt.xlabel(\"Episode\"); plt.ylabel(\"Avg Reward\"); plt.grid(True)\n",
        "    else: plt.title(f\"Avg Reward per {print_freq} Episodes (No Data)\"); plt.grid(True)\n",
        "\n",
        "    # 2. Avg Weighted Cost Plot\n",
        "    plt.subplot(num_plots // 2, 2, 2)\n",
        "    overall_costs_data = results.get(\"overall_weighted_costs\")\n",
        "    if overall_costs_data and any(not np.isnan(c) for c in overall_costs_data):\n",
        "        plt.plot(overall_costs_data)\n",
        "        plt.title(\"Avg Overall Weighted Unit Cost per Episode\"); plt.xlabel(\"Episode\"); plt.ylabel(\"Avg Weighted Unit Cost\"); plt.grid(True)\n",
        "    else: plt.title(\"Avg Overall Weighted Unit Cost (No Data)\"); plt.grid(True)\n",
        "\n",
        "    # 3. Avg Diversity Plot\n",
        "    plt.subplot(num_plots // 2, 2, 3)\n",
        "    overall_diversities_data = results.get(\"overall_diversities\")\n",
        "    if overall_diversities_data and any(not np.isnan(d) for d in overall_diversities_data):\n",
        "        plt.plot(overall_diversities_data)\n",
        "        plt.title(\"Avg Overall Diversity (Entropy) per Episode\"); plt.xlabel(\"Episode\"); plt.ylabel(\"Shannon Entropy\")\n",
        "        max_entropy = np.log(len(parameters_df)) if len(parameters_df) > 1 else 1.0\n",
        "        plt.axhline(y=max_entropy, color='r', linestyle='--', label=f'Max Entropy ({max_entropy:.2f})')\n",
        "        plt.legend(); plt.grid(True)\n",
        "    else: plt.title(\"Avg Overall Diversity (No Data)\"); plt.grid(True)\n",
        "\n",
        "    # 4. Episode Lengths Plot\n",
        "    plt.subplot(num_plots // 2, 2, 4)\n",
        "    episode_lengths_data = results.get(\"episode_lengths\")\n",
        "    if episode_lengths_data and len(episode_lengths_data) > 0:\n",
        "        plt.plot(episode_lengths_data)\n",
        "        plt.title(\"Episode Lengths\"); plt.xlabel(\"Episode\"); plt.ylabel(\"Steps\"); plt.grid(True)\n",
        "    else: plt.title(\"Episode Lengths (No Data)\"); plt.grid(True)\n",
        "\n",
        "    # 5. Losses Plot\n",
        "    plt.subplot(num_plots // 2, 2, 5)\n",
        "    policy_losses = results.get(\"policy_losses\"); value_losses = results.get(\"value_losses\")\n",
        "    updates_made = max(len(policy_losses) if policy_losses else 0, len(value_losses) if value_losses else 0)\n",
        "    if updates_made > 0:\n",
        "        updates_x = range(updates_made)\n",
        "        if policy_losses and len(policy_losses) == updates_made: plt.plot(updates_x, policy_losses, label='Policy Loss', alpha=0.7)\n",
        "        if value_losses and len(value_losses) == updates_made: plt.plot(updates_x, value_losses, label='Value Loss', alpha=0.7)\n",
        "        plt.title(\"Avg Losses During Updates\"); plt.xlabel(\"Update Cycle\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True)\n",
        "    else: plt.title(\"Avg Losses During Updates (No Data)\"); plt.grid(True)\n",
        "\n",
        "    # 6. Entropy Plot\n",
        "    plt.subplot(num_plots // 2, 2, 6)\n",
        "    entropies = results.get(\"entropies\")\n",
        "    if entropies and len(entropies) > 0:\n",
        "        plt.plot(range(len(entropies)), entropies)\n",
        "        plt.title(\"Avg Policy Entropy During Updates\"); plt.xlabel(\"Update Cycle\"); plt.ylabel(\"Entropy\"); plt.grid(True)\n",
        "    else: plt.title(\"Avg Policy Entropy (No Data)\"); plt.grid(True)\n",
        "\n",
        "    plt.suptitle(\"PPO Training Performance (RNN State)\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "def plot_evaluation_distribution(proportions_history: List[np.ndarray], parameters_df: pd.DataFrame):\n",
        "    if not proportions_history or not isinstance(proportions_history, list) or not all(isinstance(p, np.ndarray) for p in proportions_history):\n",
        "         print(\"No valid proportion history for evaluation distribution plot.\")\n",
        "         return\n",
        "    try:\n",
        "        proportions_array = np.array(proportions_history)\n",
        "        if np.isnan(proportions_array).any():\n",
        "             print(\"Warning: NaN values found in proportions_history. Replacing with 0.\")\n",
        "             proportions_array = np.nan_to_num(proportions_array)\n",
        "    except ValueError as e:\n",
        "         print(f\"Error converting proportion history to NumPy array: {e}. Check shapes.\")\n",
        "         return\n",
        "\n",
        "    num_steps, num_types = proportions_array.shape\n",
        "    if num_types != len(parameters_df): deposit_types = [f\"Type_{i}\" for i in range(num_types)]\n",
        "    else: deposit_types = parameters_df['Deposit_Type'].tolist()\n",
        "\n",
        "    steps = np.arange(num_steps)\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    try:\n",
        "        plt.stackplot(steps, proportions_array.T, labels=deposit_types, alpha=0.8)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error during stackplot: {e}\")\n",
        "        print(f\"Steps shape: {steps.shape}, Props T shape: {proportions_array.T.shape}\")\n",
        "        return\n",
        "\n",
        "    plt.title(\"Deposit Type Distribution (by Amount) During First Evaluation Episode (RNN Agent)\")\n",
        "    plt.xlabel(\"Time Step (Months)\"); plt.ylabel(\"Proportion of Total Portfolio Amount\")\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0.)\n",
        "    plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
        "    plt.xlim(0, max(1, num_steps - 1)); plt.ylim(0, 1)\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "\n",
        "\n",
        "# --- Main Execution (Adjusted Parameters for RNN) ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Configuration ---\n",
        "    SEED = 42\n",
        "    NUM_INITIAL_DEPOSITS = 50     # Start with more deposits?\n",
        "    INITIAL_MIN_AMOUNT = 500.0\n",
        "    INITIAL_MAX_AMOUNT = 20000.0\n",
        "    MAX_EPISODE_STEPS = 120       # Shorter episodes? Test if learns faster horizon\n",
        "\n",
        "    # Reward Weights (Tune these)\n",
        "    COST_WEIGHT = 0.6\n",
        "    DIVERSITY_WEIGHT = 0.4\n",
        "    ROLL_OFF_PENALTY_WEIGHT = 1.5 # Less harsh penalty?\n",
        "\n",
        "    # RNN/Embedding Parameters <<< NEW\n",
        "    EMBEDDING_DIM = 128           # Dimension of the state embedding from RNN\n",
        "    RNN_HIDDEN_DIM = 128          # LSTM hidden layer size\n",
        "    NUM_RNN_FEATURES = 4          # Current_Lifetime, Cost, Amount, Type_Index\n",
        "\n",
        "    # PPO Hyperparameters\n",
        "    PPO_ACTOR_CRITIC_HIDDEN_DIM = 128 # Hidden layer size for Actor/Critic MLPs\n",
        "    PPO_BATCH_SIZE = 128          # Smaller batch size might work with RNN?\n",
        "    PPO_UPDATE_TIMESTEP = 1024    # Update more frequently? Must be >= batch_size\n",
        "    PPO_EPOCHS = 8\n",
        "    LR_ENCODER = 3e-4             # Learning rate for RNN Encoder\n",
        "    LR_ACTOR = 3e-4\n",
        "    LR_CRITIC = 1e-3              # Critic LR often higher\n",
        "    GAMMA = 0.98                  # Discount factor\n",
        "    GAE_LAMBDA = 0.95\n",
        "    CLIP_RATIO = 0.2\n",
        "    ENTROPY_COEF = 0.02           # Slightly higher entropy bonus?\n",
        "    VALUE_COEF = 0.5\n",
        "    MAX_GRAD_NORM = 1.0           # Allow slightly larger gradients?\n",
        "\n",
        "    # Training & Evaluation Settings\n",
        "    NUM_TRAINING_EPISODES = 3000  # Fewer episodes initially for testing RNN\n",
        "    FIT_SCALER_EPISODES = 20      # Episodes to collect data for normalization scaler\n",
        "    PRINT_FREQ = 25\n",
        "    NUM_EVAL_EPISODES = 20\n",
        "    RENDER_EVALUATION = False\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Mean of empty slice\")\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*UserWarning: torch.nn.utils.rnn.pack_padded_sequence received a Tensor length.*\") # Ignore length warning\n",
        "\n",
        "    print(\"--- Starting Deposit Management Simulation (RNN-Based State) ---\")\n",
        "    # --- Seed everything ---\n",
        "    np.random.seed(SEED); torch.manual_seed(SEED); random.seed(SEED)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- Setup ---\n",
        "    parameters_df = create_parameters_df()\n",
        "    initial_df = create_data(parameters_df, num_records=NUM_INITIAL_DEPOSITS,\n",
        "                             min_amount=INITIAL_MIN_AMOUNT, max_amount=INITIAL_MAX_AMOUNT)\n",
        "\n",
        "    print(\"\\n--- Parameters ---\"); print(parameters_df)\n",
        "    initial_total_amount = initial_df['Amount'].sum()\n",
        "    print(f\"\\nInitial Records: {len(initial_df)}, Initial Total Amount: {initial_total_amount:,.2f}\")\n",
        "    print(f\"{'-'*50}\")\n",
        "\n",
        "    env = None; agent = None\n",
        "    try:\n",
        "        # --- Initialize Environment ---\n",
        "        env = DepositManagementEnv(\n",
        "            initial_df=initial_df.copy(),\n",
        "            parameters_df=parameters_df.copy(),\n",
        "            embedding_dim=EMBEDDING_DIM, # Pass embedding dim\n",
        "            max_steps=MAX_EPISODE_STEPS,\n",
        "            cost_weight=COST_WEIGHT,\n",
        "            diversity_weight=DIVERSITY_WEIGHT,\n",
        "            roll_off_penalty_weight=ROLL_OFF_PENALTY_WEIGHT,\n",
        "            render_mode=None\n",
        "            # Bucket boundaries are only used for info now\n",
        "        )\n",
        "\n",
        "        # --- Initialize Agent ---\n",
        "        action_dim = env.action_space.shape[0]\n",
        "        print(f\"\\nInitializing PPO Agent (RNN): Features={NUM_RNN_FEATURES}, Embedding Dim={EMBEDDING_DIM}, Action Dim={action_dim}\")\n",
        "        agent = PPOAgent(\n",
        "            num_rnn_features=NUM_RNN_FEATURES,\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            action_dim=action_dim,\n",
        "            rnn_hidden_dim=RNN_HIDDEN_DIM,\n",
        "            actor_critic_hidden_dim=PPO_ACTOR_CRITIC_HIDDEN_DIM,\n",
        "            lr_encoder=LR_ENCODER,\n",
        "            lr_actor=LR_ACTOR,\n",
        "            lr_critic=LR_CRITIC,\n",
        "            gamma=GAMMA, gae_lambda=GAE_LAMBDA, clip_ratio=CLIP_RATIO,\n",
        "            entropy_coef=ENTROPY_COEF, value_coef=VALUE_COEF, max_grad_norm=MAX_GRAD_NORM,\n",
        "            update_epochs=PPO_EPOCHS, batch_size=PPO_BATCH_SIZE\n",
        "        )\n",
        "        print(\"PPO Agent (RNN) created successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR during Setup: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        if env: env.close()\n",
        "        exit(1)\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    training_results = {}\n",
        "    try:\n",
        "        effective_update_timestep = max(PPO_UPDATE_TIMESTEP, PPO_BATCH_SIZE)\n",
        "        if effective_update_timestep != PPO_UPDATE_TIMESTEP:\n",
        "             print(f\"Adjusted update timestep to {effective_update_timestep}\")\n",
        "\n",
        "        training_results = train(env, agent, initial_df.copy(), NUM_TRAINING_EPISODES,\n",
        "                                 effective_update_timestep, PRINT_FREQ,\n",
        "                                 fit_scaler_episodes=FIT_SCALER_EPISODES) # Pass scaler fit episodes\n",
        "\n",
        "    except KeyboardInterrupt: print(\"\\n--- Training Interrupted ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- CRITICAL ERROR during training ---\"); print(e)\n",
        "        import traceback; traceback.print_exc()\n",
        "\n",
        "    # --- Plot Training Results ---\n",
        "    if training_results and any(isinstance(v, list) and v for v in training_results.values()):\n",
        "        print(\"\\n--- Plotting Training Results ---\")\n",
        "        try: plot_results(training_results, PRINT_FREQ, parameters_df)\n",
        "        except Exception as e: print(f\"Error plotting training results: {e}\")\n",
        "    else: print(\"\\n--- Skipping Training Plot (No Results) ---\")\n",
        "\n",
        "    # --- Evaluation Phase ---\n",
        "    print(\"\\n--- Evaluating Final Agent ---\")\n",
        "    eval_summary = {}\n",
        "    proportions_history = None\n",
        "    if agent is not None and env is not None:\n",
        "        try:\n",
        "            eval_summary, proportions_history = evaluate(\n",
        "                env, agent, initial_df.copy(), parameters_df, NUM_EVAL_EPISODES, RENDER_EVALUATION\n",
        "            )\n",
        "        except KeyboardInterrupt: print(\"\\n--- Evaluation Interrupted ---\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n--- CRITICAL ERROR during evaluation ---\"); print(e)\n",
        "            import traceback; traceback.print_exc()\n",
        "    else: print(\"Skipping evaluation (Agent/Env not initialized).\")\n",
        "\n",
        "    # --- Plot Evaluation Distribution ---\n",
        "    if proportions_history:\n",
        "        print(\"\\n--- Plotting Evaluation Distribution (First Episode) ---\")\n",
        "        try: plot_evaluation_distribution(proportions_history, parameters_df)\n",
        "        except Exception as e: print(f\"Error plotting evaluation distribution: {e}\")\n",
        "    else: print(\"\\n--- Skipping Evaluation Distribution Plot ---\")\n",
        "\n",
        "    # --- Display Plots ---\n",
        "    if (training_results and any(isinstance(v, list) and v for v in training_results.values())) or proportions_history:\n",
        "        print(\"\\nDisplaying generated plots...\")\n",
        "        plt.show()\n",
        "    else: print(\"\\nNo plots to display.\")\n",
        "\n",
        "    # --- Cleanup ---\n",
        "    try:\n",
        "        if env: env.close()\n",
        "    except Exception as e: print(f\"Error closing environment: {e}\")\n",
        "\n",
        "    print(\"\\n--- Script Finished ---\")"
      ]
    }
  ]
}