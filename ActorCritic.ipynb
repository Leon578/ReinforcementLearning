{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B4g3GsN6mZcM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_probability as tfp \n",
        "from keras import layers\n",
        "from keras.models import Model \n",
        "from keras.optimizers import Adam\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  def __init__(self, num_actions=2, num_hidden_units=1024):\n",
        "    super(ActorCritic, self).__init__() \n",
        "    self.num_actions = num_actions\n",
        "    self.shared_1 = layers.Dense(num_hidden_units, activation = \"relu\")\n",
        "    self.shared_2 = layers.Dense(num_hidden_units, activation = \"relu\")\n",
        "    self.actor = layers.Dense(num_actions, \"softmax\")\n",
        "    self.critic = layers.Dense(1, None)\n",
        "\n",
        "  def call(self, input_obs):\n",
        "    x = self.shared_1(input_obs)\n",
        "    x = self.shared_2(x)\n",
        "    v = self.critic(x)\n",
        "    pol = self.actor(v)\n",
        "    return v, pol\n"
      ],
      "metadata": {
        "id": "AUAmzhLpmigl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, num_actions=2, alpha = 0.001, gamma = 0.99):\n",
        "    self.gamma = gamma\n",
        "    self.aplha = alpha \n",
        "    self.num_actions = num_actions \n",
        "    self.action = None\n",
        "    self.network = ActorCritic(num_actions=self.num_actions)\n",
        "    self.network.compile(optimizer = Adam(learning_rate = alpha))\n",
        "    self.optimizer = Adam(learning_rate = alpha)\n",
        "  def choose_action(self, obs): \n",
        "    tf_obs = tf.convert_to_tensor([obs])\n",
        "    _, action_logits = self.network.call(tf_obs)\n",
        "    discrete_dist = tfp.distributions.Categorical(logits=action_logits)\n",
        "    action = discrete_dist.sample() #This returns a tensor, \n",
        "    self.action = action\n",
        "    return action.numpy()[0]\n",
        "  \n",
        "  def save_models(self):\n",
        "    print(\">>>>savingModel<<<<\")\n",
        "    self.network.save_weights(\"actorcritic.h5\")\n",
        "\n",
        "  def load_models(self):\n",
        "    print(\">>>>Load Model<<<<\")\n",
        "    self.network.load_weights(\"actorcritic.h5\")\n",
        "  \n",
        "  def learn(self, state, reward, next_state, done):\n",
        "    tf_state = tf.convert_to_tensor([state], dtype = tf.float32)\n",
        "    tf_reward = tf.convert_to_tensor(reward, dtype = tf.float32)\n",
        "    tf_next_state = tf.convert_to_tensor([next_state], dtype = tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      state_val, action_logits = self.network.call(tf_state)\n",
        "      next_state_val, next_action_logits = self.network.call(tf_next_state)\n",
        "\n",
        "      state_val = tf.squeeze(state_val)\n",
        "      next_state_val = tf.squeeze(next_state_val)\n",
        "      delta = tf_reward + self.gamma*next_state_val*(1-int(done)) - state_val  #int(done) == 0 when the round is not done\n",
        "\n",
        "      loss_critic = delta**2 \n",
        "      discrete_dist = tfp.distributions.Categorical(logits=action_logits)\n",
        "      log_prob = discrete_dist.log_prob(self.action)\n",
        "\n",
        "      loss_actor = -log_prob*delta\n",
        "      total_loss = loss_actor + loss_critic\n",
        "      grads = tape.gradient(total_loss, self.network.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.network.trainable_variables))"
      ],
      "metadata": {
        "id": "urX-YEtAoRnh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5MtJlSVwDCT",
        "outputId": "8afa273f-3f3b-4e83-865c-f654d64b3b35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 2000 \n",
        "agent = Agent()\n",
        "for i in range(eps):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  total_reward = 0\n",
        "  while not done:\n",
        "    action = agent.choose_action(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    agent.learn(state, reward, next_state, done)\n",
        "    state = next_state\n",
        "\n",
        "  if i%20 == 0: \n",
        "    print(total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv8RaGMBwgx5",
        "outputId": "17932429-9f81-403e-8384-fda7941b5d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19.0\n",
            "60.0\n",
            "18.0\n",
            "22.0\n",
            "16.0\n",
            "23.0\n",
            "19.0\n",
            "17.0\n",
            "30.0\n",
            "16.0\n",
            "11.0\n"
          ]
        }
      ]
    }
  ]
}