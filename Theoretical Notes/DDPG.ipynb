{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#***Why we can't use Q-learning for continouos action Spaces?*** \n",
        "\n",
        "\n",
        "(References\n",
        "\n",
        " https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id3\n",
        " \n",
        " https://stackoverflow.com/questions/59891158/what-are-target-network-in-policy-gradient-algorithms-in-reinforcement-learning\n",
        "\n",
        " https://arxiv.org/pdf/1509.02971v6.pdf\n",
        " )\n",
        "\n",
        "\n",
        "We cannot use Q-learning for finding the best action in a continous setting, since usually we only have a discrete number of actions and it is easy to compare this discrete number of outputs to each other and pick the best one. \n",
        "\n",
        "But it is clear that if we have a continous action space then we can take the derivative of Q(S,A) with respect to the actions. If we are able to do this, then we would be able to calculate the maximum of the Q-value of a given state by just taking this derivative and setting it equal to zero. Here we have that the output of the policy network (The network which chooses the action) will be the input to the Q-network as well as the current state. \n",
        "\n",
        "Thus since the Policy network's output is the input to the Q-network, this means that taking a derivative with respect to the action and taking a derivative with respect to the Policy network is the same, both mean, if we change the actions, how will the value change? \n",
        "\n",
        "So we want to find maxᵦE[Q(s,μᵦ(s))], where μᵦ is our policy and we want to find the policy which on average chooses the actions that give the highest rewards. Since μᵦ has a contiuous output we can find a derivative of the Q-network with respect to our actions. \n",
        "\n",
        "There is also what we call ***Curse of Dimentionality*** which if we discretize the output space, then we will have that there will be a much larger acton space, say we have a robotic arm which can controll 7 joints, then if we have 3 actions for each joint we have a action space size of 3^7. "
      ],
      "metadata": {
        "id": "gz_3nCGkES6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Mean Squared Bellmann Error***\n",
        "\n",
        "L(ϕ) = (Qᵩ(s, a) - (R(s,a,s') + γ*(max\\_{a'}Qᵩ(s',a'))))\\^2\n",
        "\n",
        "This as we know is unstable, and we can use doule q-learning to improve this process in Deep-Q-Learning, where some traget network is updated to be equal to the current network after some fixed number of steps (See the section I wrote under Double-Q-Learning).\n",
        "\n",
        "\n",
        "In DDPG style algorithms we can have some soft-assignment with every update of the network, where we have that \n",
        "\n",
        "Qₜ <- ρQᵩ+(1-ρ)Qₜ\n",
        "\n",
        "Where Qₜ is the target network."
      ],
      "metadata": {
        "id": "LluRhfDTI5lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Experience ReplayBuffers***\n",
        "\n",
        "Use these to store past experiences, such that we have a pool of past expereinces to sample from. Don't just throw away the most out of date experiences, as this will imply that the model may overfit. \n",
        "\n",
        "Using Expericne replay buffers, we also make get rid of correlation between the samples. The correlation between samples is for example, if we are following a policy that goes to a similar state at every time-step then the difference between the the updated and the current Q-values will be less and less(TD learning) this action can become \"overly reinforced\", thus using an experience replay buffer uncorrelates these observations, since it uses data from past states to learn, putting the sequence of similar states into a better context with that states which we have already seen. \n",
        "\n",
        "We give the model some extra training data every time, such that it can use these experiences and their rewards to learn from different policies.\n",
        "\n",
        "Using DDPG algorithms, we should note that we have an off-policy learning algorithm, so it is fine to have observations, even if they are not from the same policy that we are following. \n",
        "\n",
        "We cannot use replay buffers with on-policy learning, this is because we had some policy and we were following it and we used this to create some memories, if we now use this in an updated policy this will not work(Elaborate on this).\n"
      ],
      "metadata": {
        "id": "tzcQt8-8KXtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Why do we use target Networks?*** \n",
        "\n",
        "The Target network is used to calculate some TD error as can be seen below. This is used for giving consistent targets for learning during backups. \n",
        "\n",
        "With the DDPG algorithm we have that the Mean Squared Bellmann Error is calculated as \n",
        "\n",
        "L(ϕ) = (Qᵩ(s, a) - (R(s,a,s') + γ*(max_{a'}Qₜ(s',a'))))^2\n",
        "\n",
        "noting again that Qₜ is the target network.\n",
        "\n",
        "Now we know wha errors are calculated for right? We want errors to be calculated such that we can minimize these errors, and we know that at each time step in the DDPG our target policy only changes by a small amount and we know that our Agent gets updated by(this loss above) Which will be much larger than the update of the traget network, meaning we should conclude that we want our Agend, or Q to be closer to the target Network. \n",
        "\n",
        "If the target-network and the Agent(Q) are the same, then there will be instabilities in training"
      ],
      "metadata": {
        "id": "0ootlhSfLUd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# ***Why do we say that the DDPG has a deterministic policy?*** \n",
        "\n",
        "If you recall how we used the other Policy gradient algorithms. We usually had a policy network that has a discrete set of outputs, each node indicating an action and each output indicating the probability of selecting that action. In the case of DDPG we only have one-output on the policy network (This value indicated the action being taken), this will mean that we will always select our action from this node such that for the same state this node will have the same value. Thus in order to get some different output/action from the same state we add noise to the output. The distribution of noise we will add will be sampled from the gaussian(0,1) distribution. "
      ],
      "metadata": {
        "id": "eslMM5jiSQ2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oTEu4QaO2LOI"
      }
    }
  ]
}