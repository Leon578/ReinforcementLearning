{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Why we can't use Q-learning for continouos action Spaces?*** \n",
        "\n",
        "\n",
        "(Reference https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id3)\n",
        "\n",
        "\n",
        "We cannot use Q-learning for finding the best action in a continous setting, since usually we only have a discrete number of actions and it is easy to compare this discrete number of outputs to each other and pick the best one. \n",
        "\n",
        "But it is clear that if we have a continous action space then we can take the derivative of Q(S,A) with respect to the actions. If we are able to do this, then we would be able to calculate the maximum of the Q-value of a given state by just taking this derivative and setting it equal to zero. Here we have that the output of the policy network (The network which chooses the action) will be the input to the Q-network as well as the current state. \n",
        "\n",
        "Thus since the Policy network's output is the input to the Q-network, this means that taking a derivative with respect to the action and taking a derivative with respect to the Policy network is the same, both mean, if we change the actions, how will the value change? \n",
        "\n",
        "So we want to find maxᵦE[Q(s,μᵦ(s))], where μᵦ is our policy and we want to find the policy which on average chooses the actions that give the highest rewards. Since μᵦ has a contiuous output we can find a derivative of the Q-network with respect to our actions. "
      ],
      "metadata": {
        "id": "gz_3nCGkES6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Mean Squared Bellmann Error***\n",
        "\n",
        "L(ϕ) = (Qᵩ(s, a) - (R(s,a,s') + γ*(max\\_{a'}Qᵩ(s',a'))))\\^2\n",
        "\n",
        "This as we know is unstable, and we can use doule q-learning to improve this process in Deep-Q-Learning, where some traget network is updated to be equal to the current network after some fixed number of steps (See the section I wrote under Double-Q-Learning).\n",
        "\n",
        "\n",
        "In DDPG style algorithms we can have some soft-assignment with every update of the network, where we have that \n",
        "\n",
        "Qₜ <- ρQᵩ+(1-ρ)Qₜ\n",
        "\n",
        "Where Qₜ is the target network."
      ],
      "metadata": {
        "id": "LluRhfDTI5lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Experience ReplayBuffers***\n",
        "\n",
        "Use these to store past experiences, such that we have a pool of past expereinces to sample from. Don't just throw away the most out of date experiences, as this will imply that the model may overfit. \n",
        "\n",
        "Using DDPG algorithms, we should note that we have an off-policy learning algorithm, so it is fine to have observations, even if they are not from the same policy that we are following. \n",
        "\n",
        "Not enoough Intuition please add more ***What is the intuition behind why Buffers are different for on-policy and off policy algorithms?***\n"
      ],
      "metadata": {
        "id": "tzcQt8-8KXtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the DDPG algorithm we have that the Mean Squared Bellmann Error is calculated as \n",
        "\n",
        "L(ϕ) = (Qᵩ(s, a) - (R(s,a,s') + γ*(max_{a'}Qₜ(s',a'))))^2\n",
        "\n",
        "noting again that Qₜ is the target network."
      ],
      "metadata": {
        "id": "0ootlhSfLUd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Why do we say that the DDPG has a deterministic policy?*** \n",
        "\n",
        "If you recall how we used the other Policy gradient algorithms. We usually had a policy network that has a discrete set of outputs, each node indicating an action and each output indicating the probability of selecting that action. In the case of DDPG we only have one-output on the policy network (This value indicated the action being taken), this will mean that we will always select our action from this node such that for the same state this node will have the same value. Thus in order to get some different output/action from the same state we add noise to the output. The distribution of noise we will add will be sampled from the gaussian(0,1) distribution. "
      ],
      "metadata": {
        "id": "eslMM5jiSQ2V"
      }
    }
  ]
}