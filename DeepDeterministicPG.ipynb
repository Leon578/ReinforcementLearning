{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B6uZxdQC2EwP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_probability as tfp \n",
        "from keras import layers\n",
        "from keras.models import Model \n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import gym\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using this notebook to show how we can use reinfrcement learning for continuous action space applications, ones in which there are a need for finely tuning actions to be just right. A use for this might be in automated market makers where we want to use a specific volume of capital to make an order. \n"
      ],
      "metadata": {
        "id": "IIqiztgd2ISX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(): \n",
        "  def __init__(self, memory_size, input_shape): \n",
        "    self.memory_size = memory_size \n",
        "    self.memory_counter = 0 \n",
        "    self.state_shape = input_shape\n",
        "    self.state_memory = np.empty((50,self.state_shape))\n",
        "    self.next_state_memory = np.empty((50,self.state_shape))\n",
        "    self.action_memory = np.empty(50)\n",
        "    self.reward_memory = np.empty(50) #This will be an array of all the memories\n",
        "    self.terminal_memory = np.empty(50)\n",
        "\n",
        "  def sample_memories(self, sample_size): \n",
        "    state = np.empty((sample_size, self.state_shape))\n",
        "    next_state = np.empty((sample_size, self.state_shape))\n",
        "    rewards = np.empty(sample_size)\n",
        "    actions = np.empty(sample_size)\n",
        "    terminal = np.empty(sample_size)\n",
        "    for i in range(sample_size):\n",
        "      rand_ind = np.random.randint(self.memory_size) #Find a random index from which we want to sample\n",
        "      state[i] = self.state_memory[rand_ind]\n",
        "      next_state[i]  = self.next_state_memory[rand_ind]\n",
        "      rewards[i]  = self.reward_memory[rand_ind]\n",
        "      actions[i]  = self.action_memory[rand_ind]\n",
        "      terminal[i]  = self.terminal_memory[rand_ind]\n",
        "    return state, next_state, rewards, actions, terminal\n",
        "  \n",
        "  def remove_memories(self): \n",
        "    rand_ind = np.random.randint(self.memory_size-1) #this -1 will just be to keep it within bounds,\n",
        "  # makingit almost completely uniform random but not quite, at least we always shift the matrix by 1 every time.\n",
        "    self.state_memory[rand_ind:] = np.concatenate((self.state_memory[rand_ind+1:], np.zeros_like(self.state_memory[-1:])), axis = 0)\n",
        "    self.next_state_memory[rand_ind:] = np.concatenate((self.next_state_memory[rand_ind+1:], np.zeros_like(self.next_state_memory[-1:])), axis = 0)\n",
        "    self.reward_memory[rand_ind:] = np.concatenate((self.reward_memory[rand_ind+1:], np.zeros_like(self.reward_memory[-1:])), axis = 0)\n",
        "    self.action_memory[rand_ind:] = np.concatenate((self.action_memory[rand_ind+1:], np.zeros_like(self.action_memory[-1:])), axis = 0)\n",
        "    self.terminal_memory[rand_ind:] = np.concatenate((self.terminal_memory[rand_ind+1:], np.zeros_like(self.terminal_memory[-1:])), axis = 0)\n",
        "  \n",
        "  def add_memories(self, state, next_state, action, reward, done):\n",
        "    self.state_memory[self.memory_size-1] = state\n",
        "    self.next_state_memory[self.memory_size-1] = next_state\n",
        "    self.reward_memory[self.memory_size-1] = reward\n",
        "    self.action_memory[self.memory_size-1] = action\n",
        "    self.terminal_memory[self.memory_size-1] = int(done)\n",
        "\n",
        "  def start_adding_memories(self, state, next_state, action, reward, done, counter):\n",
        "    self.state_memory[counter] = state\n",
        "    self.next_state_memory[counter] = next_state\n",
        "    self.reward_memory[counter] = reward\n",
        "    self.action_memory[counter] = action\n",
        "    self.terminal_memory[counter] = int(done)\n"
      ],
      "metadata": {
        "id": "OfXVRkDA2iis"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(tf.keras.Model): \n",
        "  def __init__(self, num_hidden_units = 1024):\n",
        "    super(Actor, self).__init__()\n",
        "    self.shared_1 = layers.Dense(num_hidden_units, activation = \"relu\")\n",
        "    self.shared_2 = layers.Dense(num_hidden_units, activation = \"relu\")\n",
        "    self.actor = layers.Dense(1, \"linear\")\n",
        "  \n",
        "  def call(self, state): \n",
        "    x = self.shared_1(state)\n",
        "    x = self.shared_2(x)\n",
        "    action = self.actor(x)\n",
        "    return action"
      ],
      "metadata": {
        "id": "GcJwEzF5Wh74"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(tf.keras.Model): \n",
        "  def __init__(self, num_hidden_units = 1024):\n",
        "    super(Critic, self).__init__()\n",
        "    self.shared_1 = layers.Dense(num_hidden_units, activation = \"relu\")\n",
        "    self.shared_2 = layers.Dense(num_hidden_units, activation = \"relu\")\n",
        "    self.critic = layers.Dense(1, \"linear\")\n",
        "\n",
        "  def call(self, state, action):\n",
        "    x = self.shared_1(tf.concat([state, action], axis = 1))\n",
        "    x = self.shared_2(x)\n",
        "    value = self.critic(x)\n",
        "    return value\n",
        "  \n"
      ],
      "metadata": {
        "id": "OtZteA0pYU3g"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(): \n",
        "  def __init__(self, input_shape, max_action_val=1, min_action_val=-1, alpha = 0.001, gamma = 0.99, rho = 0.005):\n",
        "    self.gamma = gamma\n",
        "    self.aplha = alpha \n",
        "    self.rho = rho\n",
        "    self.actor_network = Actor()  \n",
        "    self.target_actor_network = Actor() \n",
        "    self.target_actor_network.set_weights(self.actor_network.get_weights())\n",
        "    self.critic_network = Critic()\n",
        "    self.target_critic_network = Critic() \n",
        "    self.target_critic_network.set_weights(self.critic_network.get_weights())\n",
        "    self.max_action_val = max_action_val\n",
        "    self.min_action_val = min_action_val\n",
        "    self.optimizer = Adam(learning_rate = alpha)\n",
        "    self.buff = ReplayBuffer(memory_size = 50, input_shape = 4)\n",
        "\n",
        "  def choose_action(self, state): \n",
        "    tf_obs = tf.convert_to_tensor([state])\n",
        "    rn = np.random.normal()\n",
        "    action = self.actor_network.call(tf_obs)[0][0]\n",
        "    action = tf.clip_by_value(action + rn, self.max_action_val, self.min_action_val)\n",
        "    \n",
        "    return action.numpy()\n",
        "\n",
        "  def learn(self, done):\n",
        "    states, next_states, rewards, actions, terminal = self.buff.sample_memories(10)\n",
        "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "    next_states = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
        "    rewards = tf.convert_to_tensor(rewards,dtype=tf.float32)\n",
        "    actions = tf.convert_to_tensor(actions,dtype=tf.float32)\n",
        "    terminal =  tf.convert_to_tensor(terminal,dtype=tf.float32)\n",
        "  \n",
        "    with tf.GradientTape() as tape: \n",
        "      next_actions = self.target_actor_network.call(next_states)\n",
        "      target_val = self.target_critic_network.call(next_states, next_actions)\n",
        "      val = self.critic_network.call(next_states, next_actions)\n",
        "      target = rewards + self.gamma*(1-int(done))*(target_val) \n",
        "      critic_loss = (target-val)**2\n",
        "      total_critic_loss = tf.reduce_mean(critic_loss) \n",
        "\n",
        "    critic_grads = tape.gradient(total_critic_loss, self.critic_network.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(critic_grads, self.critic_network.trainable_variables))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      other_actions = self.actor_network.call(states)\n",
        "      other_vals = self.critic_network.call(states, other_actions)\n",
        "      actor_loss = -tf.reduce_mean(other_vals)#We will perform gradient ascent\n",
        "    actor_grads = tape.gradient(actor_loss, self.actor_network.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(actor_grads, self.actor_network.trainable_variables))\n",
        "\n",
        "    self.update()\n",
        "\n",
        "  def update(self): \n",
        "    critic_weights = self.critic_network.get_weights()\n",
        "    curr_critic_target_weights = self.target_critic_network.get_weights()\n",
        "    critic_target_weights = []\n",
        "    for i in range(len(critic_weights)):\n",
        "      critic_target_weights.append((1-self.rho) * curr_critic_target_weights[i] + self.rho *  critic_weights[i])\n",
        "\n",
        "    self.target_critic_network.set_weights(critic_target_weights)\n",
        "\n",
        "    actor_weights = self.actor_network.get_weights()\n",
        "    curr_actor_target_weights = self.target_actor_network.get_weights()\n",
        "    actor_target_weights = []\n",
        "    for i in range(len(actor_weights)):\n",
        "      actor_target_weights.append((1-self.rho) * curr_actor_target_weights[i] + self.rho *  actor_weights[i])\n",
        "\n",
        "    self.target_actor_network.set_weights(actor_target_weights)\n"
      ],
      "metadata": {
        "id": "RuoN7wdugbSr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E7M_ApFeTAy",
        "outputId": "d985f052-bb3f-4290-f067-5d597880e69d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Admittedly I need to find a more appropraite env to run this on, this env is only used since it is the env I am using throughoutall the code. You will see some rounding taking place which is not ideal. "
      ],
      "metadata": {
        "id": "KSmmYBduCj92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agent = Agent(input_shape = 4)\n",
        "added = 0\n",
        "for i in range(2000): \n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  total_reward = 0\n",
        "  while not done: \n",
        "    action = agent.choose_action(state)\n",
        "    next_state, reward, done, _ = env.step(action.astype(np.int32))\n",
        "    total_reward+= reward\n",
        "    #update the policy and the q-network\n",
        "    if added >= 50:\n",
        "      agent.buff.remove_memories()\n",
        "      agent.buff.add_memories(state, next_state, action, reward, done)\n",
        "      agent.learn(done)\n",
        "    else: \n",
        "      agent.buff.start_adding_memories(state, next_state, action, reward, done, added)\n",
        "      added = added +1\n",
        "\n",
        "    state = next_state\n",
        "  if i%20 == 0: \n",
        "    print(total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zuS_VzEeF5d",
        "outputId": "937c73f1-64bd-4092-b567-c43dad2cd272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.0\n",
            "11.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "9.0\n",
            "10.0\n"
          ]
        }
      ]
    }
  ]
}